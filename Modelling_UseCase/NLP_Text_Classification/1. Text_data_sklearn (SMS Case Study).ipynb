{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Text Data and Naive Bayes in scikit-learn\n",
    "\n",
    "    Problem: we need to classify wheather a SMS is spam or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda\n",
    "\n",
    "**Working with text data**\n",
    "\n",
    "- Representing text as data\n",
    "- Reading SMS data\n",
    "- Vectorizing SMS data\n",
    "- Examining the tokens and their counts\n",
    "- Bonus: Calculating the \"spamminess\" of each token\n",
    "\n",
    "**Naive Bayes classification**\n",
    "\n",
    "- Building a Naive Bayes model\n",
    "- Comparing Naive Bayes with logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Representing text as data\n",
    "\n",
    "From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "> Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect **numerical feature vectors with a fixed size** rather than the **raw text documents with variable length**.\n",
    "\n",
    "We will use [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to \"convert text into a matrix of token counts\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with a simple example\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['call you tonight', 'Call me a cab', 'please call me... PLEASE!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cab',\n",
       " 'call',\n",
       " 'call me',\n",
       " 'call you',\n",
       " 'me',\n",
       " 'me cab',\n",
       " 'me please',\n",
       " 'please',\n",
       " 'please call',\n",
       " 'tonight',\n",
       " 'you',\n",
       " 'you tonight']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learn the 'vocabulary' of the training data\n",
    "vect = CountVectorizer(ngram_range=(1, 2),binary=True)\n",
    "vect.fit(simple_train)\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x12 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 16 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform training data into a 'document-term matrix'\n",
    "simple_train_dtm = vect.transform(simple_train)\n",
    "simple_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 8)\t1\n"
     ]
    }
   ],
   "source": [
    "# print the sparse matrix\n",
    "print(simple_train_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "       [1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sparse matrix to a dense matrix\n",
    "simple_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>call me</th>\n",
       "      <th>call you</th>\n",
       "      <th>me</th>\n",
       "      <th>me cab</th>\n",
       "      <th>me please</th>\n",
       "      <th>please</th>\n",
       "      <th>please call</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "      <th>you tonight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  call me  call you  me  me cab  me please  please  please call  \\\n",
       "0    0     1        0         1   0       0          0       0            0   \n",
       "1    1     1        1         0   1       1          0       0            0   \n",
       "2    0     1        1         0   1       0          1       1            1   \n",
       "\n",
       "   tonight  you  you tonight  \n",
       "0        1    1            1  \n",
       "1        0    0            0  \n",
       "2        0    0            0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "import pandas as pd\n",
    "pd.DataFrame(simple_train_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "> In this scheme, features and samples are defined as follows:\n",
    "\n",
    "> - Each individual token occurrence frequency (normalized or not) is treated as a **feature**.\n",
    "> - The vector of all the token frequencies for a given document is considered a multivariate **sample**.\n",
    "\n",
    "> A **corpus of documents** can thus be represented by a matrix with **one row per document** and **one column per token** (e.g. word) occurring in the corpus.\n",
    "\n",
    "> We call **vectorization** the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the **Bag of Words** or \"Bag of n-grams\" representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data into a document-term matrix (using existing vocabulary)\n",
    "simple_test = [\"please don't call me\"]\n",
    "simple_test_dtm = vect.transform(simple_test)\n",
    "simple_test_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>call me</th>\n",
       "      <th>call you</th>\n",
       "      <th>me</th>\n",
       "      <th>me cab</th>\n",
       "      <th>me please</th>\n",
       "      <th>please</th>\n",
       "      <th>please call</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "      <th>you tonight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  call me  call you  me  me cab  me please  please  please call  \\\n",
       "0    0     1        1         0   1       0          0       1            0   \n",
       "\n",
       "   tonight  you  you tonight  \n",
       "0        0    0            0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "pd.DataFrame(simple_test_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "- `vect.fit(train)` learns the vocabulary of the training data\n",
    "- `vect.transform(train)` uses the fitted vocabulary to build a document-term matrix from the training data\n",
    "- `vect.transform(test)` uses the fitted vocabulary to build a document-term matrix from the testing data (and ignores tokens it hasn't seen before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Reading SMS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms = pd.read_csv('C:/Users/om/Downloads/Text mining & NLP files/sms case study/sms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm gonna be home soon and i don't want to tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>spam</td>\n",
       "      <td>SIX chances to win CASH! From 100 to 20,000 po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT! You have won a 1 week FREE membership ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>spam</td>\n",
       "      <td>XXXMobileMovieClub: To use your credit, click ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ham</td>\n",
       "      <td>Oh k...i'm watching here:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ham</td>\n",
       "      <td>Eh u remember how 2 spell his name... Yes i di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ham</td>\n",
       "      <td>Fine if thats the way u feel. Thats the way ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>spam</td>\n",
       "      <td>England v Macedonia - dont miss the goals/team...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            message\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5   spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6    ham  Even my brother is not like to speak with me. ...\n",
       "7    ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8   spam  WINNER!! As a valued network customer you have...\n",
       "9   spam  Had your mobile 11 months or more? U R entitle...\n",
       "10   ham  I'm gonna be home soon and i don't want to tal...\n",
       "11  spam  SIX chances to win CASH! From 100 to 20,000 po...\n",
       "12  spam  URGENT! You have won a 1 week FREE membership ...\n",
       "13   ham  I've been searching for the right words to tha...\n",
       "14   ham                I HAVE A DATE ON SUNDAY WITH WILL!!\n",
       "15  spam  XXXMobileMovieClub: To use your credit, click ...\n",
       "16   ham                         Oh k...i'm watching here:)\n",
       "17   ham  Eh u remember how 2 spell his name... Yes i di...\n",
       "18   ham  Fine if thats the way u feel. Thats the way ...\n",
       "19  spam  England v Macedonia - dont miss the goals/team..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.shape\n",
    "\n",
    "sms.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert label to a numeric variable\n",
    "sms['label'] = sms.label.map({'ham':0, 'spam':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y\n",
    "X = sms.message\n",
    "y = sms.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179,)\n",
      "(1393,)\n",
      "(1393,)\n",
      "(4179,)\n"
     ]
    }
   ],
   "source": [
    "# split into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Vectorizing SMS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(strip_accents='unicode',stop_words='english',max_df=0.9,min_df=0.001)  \n",
    "\n",
    "# max_df,min_df control the number of term/feature in Dataframe\n",
    "# max_features= 2000, give top 2000 feature based on occuring\n",
    "# max_df=0.9,min_df=0.001 , max_feature =1000, first apply max_df and min_df and then apply max_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVectorizer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x1279 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 23463 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learn training data vocabulary, then create document-term matrix\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x1279 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 23463 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alternative: combine fit and transform into a single step\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1393x1279 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7734 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Examining the tokens and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store token names\n",
    "X_train_tokens = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '03', '04', '0800', '08000839402', '08000930705', '0870', '08707509020', '08712300220', '08712460324', '10', '100', '1000', '10am', '10p', '11', '11mths', '12', '12hrs', '1327', '150', '150p', '150pm', '150ppm', '16', '18', '1st', '20', '200', '2000', '2003', '20p', '21', '25', '250', '25p', '2day', '2lands', '2nd', '2nite', '30', '3030', '350', '36504', '3g', '40gb', '4th', '4u', '50']\n"
     ]
    }
   ],
   "source": [
    "# first 50 tokens\n",
    "print(X_train_tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wins', 'wish', 'wishes', 'wishing', 'wit', 'wiv', 'wk', 'wkly', 'woke', 'won', 'wonder', 'wonderful', 'wondering', 'wont', 'word', 'words', 'work', 'working', 'works', 'world', 'worried', 'worries', 'worry', 'worse', 'worth', 'wot', 'wow', 'write', 'wrong', 'www', 'xmas', 'xx', 'xxx', 'xy', 'ya', 'yar', 'yay', 'yeah', 'year', 'years', 'yep', 'yes', 'yest', 'yesterday', 'ym', 'yo', 'yr', 'yrs', 'yup', 'zed']\n"
     ]
    }
   ],
   "source": [
    "# last 50 tokens\n",
    "print(X_train_tokens[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view X_train_dtm as a dense matrix\n",
    "X_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 23,  6, ...,  6, 33,  6], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count how many times EACH token appears across ALL messages in X_train_dtm\n",
    "import numpy as np\n",
    "X_train_counts = np.sum(X_train_dtm.toarray(), axis=0)\n",
    "X_train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1279,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0800</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>yo</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>yr</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>yrs</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>yup</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>zed</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1279 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     token  count\n",
       "0       00      5\n",
       "1      000     23\n",
       "2       03      6\n",
       "3       04      9\n",
       "4     0800     10\n",
       "...    ...    ...\n",
       "1274    yo     23\n",
       "1275    yr     11\n",
       "1276   yrs      6\n",
       "1277   yup     33\n",
       "1278   zed      6\n",
       "\n",
       "[1279 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a DataFrame of tokens with their counts\n",
    "pd.DataFrame({'token':X_train_tokens, 'count':X_train_counts})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Calculating the \"spamminess\" of each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create separate DataFrames for ham and spam\n",
    "sms_ham = sms[sms.label==0]\n",
    "sms_spam = sms[sms.label==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '02',\n",
       " '03',\n",
       " '04',\n",
       " '06',\n",
       " '0800',\n",
       " '08000839402',\n",
       " '08000930705',\n",
       " '0870',\n",
       " '08707509020',\n",
       " '08712300220',\n",
       " '08712460324',\n",
       " '08718720201',\n",
       " '09050090044',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10am',\n",
       " '10p',\n",
       " '11',\n",
       " '11mths',\n",
       " '12',\n",
       " '12hrs',\n",
       " '1327',\n",
       " '150',\n",
       " '150p',\n",
       " '150pm',\n",
       " '150ppm',\n",
       " '16',\n",
       " '18',\n",
       " '1st',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2003',\n",
       " '2004',\n",
       " '20p',\n",
       " '25',\n",
       " '250',\n",
       " '25p',\n",
       " '28',\n",
       " '2day',\n",
       " '2lands',\n",
       " '2nd',\n",
       " '2nite',\n",
       " '30',\n",
       " '3030',\n",
       " '350',\n",
       " '36504',\n",
       " '3g',\n",
       " '40gb',\n",
       " '4th',\n",
       " '4u',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '50p',\n",
       " '5wb',\n",
       " '5we',\n",
       " '62468',\n",
       " '750',\n",
       " '7pm',\n",
       " '800',\n",
       " '8007',\n",
       " '82277',\n",
       " '85023',\n",
       " '86021',\n",
       " '86688',\n",
       " '87066',\n",
       " '8th',\n",
       " '900',\n",
       " 'aathi',\n",
       " 'abiola',\n",
       " 'able',\n",
       " 'abt',\n",
       " 'ac',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'account',\n",
       " 'activate',\n",
       " 'actually',\n",
       " 'add',\n",
       " 'added',\n",
       " 'address',\n",
       " 'admirer',\n",
       " 'advance',\n",
       " 'aft',\n",
       " 'afternoon',\n",
       " 'aftr',\n",
       " 'age',\n",
       " 'age16',\n",
       " 'ago',\n",
       " 'ah',\n",
       " 'aha',\n",
       " 'ahead',\n",
       " 'ahmad',\n",
       " 'aight',\n",
       " 'air',\n",
       " 'aiyah',\n",
       " 'aiyo',\n",
       " 'al',\n",
       " 'alex',\n",
       " 'alright',\n",
       " 'alrite',\n",
       " 'amp',\n",
       " 'angry',\n",
       " 'announcement',\n",
       " 'ans',\n",
       " 'answer',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anytime',\n",
       " 'apartment',\n",
       " 'app',\n",
       " 'apply',\n",
       " 'appreciate',\n",
       " 'april',\n",
       " 'ar',\n",
       " 'ard',\n",
       " 'area',\n",
       " 'armand',\n",
       " 'arrange',\n",
       " 'arrive',\n",
       " 'asap',\n",
       " 'ask',\n",
       " 'askd',\n",
       " 'asked',\n",
       " 'askin',\n",
       " 'asking',\n",
       " 'asleep',\n",
       " 'ass',\n",
       " 'ate',\n",
       " 'attempt',\n",
       " 'auction',\n",
       " 'available',\n",
       " 'ave',\n",
       " 'await',\n",
       " 'awaiting',\n",
       " 'awake',\n",
       " 'award',\n",
       " 'awarded',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'b4',\n",
       " 'babe',\n",
       " 'babes',\n",
       " 'baby',\n",
       " 'bad',\n",
       " 'bag',\n",
       " 'bak',\n",
       " 'balance',\n",
       " 'bank',\n",
       " 'barely',\n",
       " 'bath',\n",
       " 'bathe',\n",
       " 'battery',\n",
       " 'bb',\n",
       " 'bcoz',\n",
       " 'beautiful',\n",
       " 'bed',\n",
       " 'bedroom',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'better',\n",
       " 'big',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'biz',\n",
       " 'black',\n",
       " 'blood',\n",
       " 'blue',\n",
       " 'bluetooth',\n",
       " 'body',\n",
       " 'bold',\n",
       " 'bonus',\n",
       " 'boo',\n",
       " 'book',\n",
       " 'booked',\n",
       " 'bored',\n",
       " 'boss',\n",
       " 'bother',\n",
       " 'bought',\n",
       " 'bout',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'boytoy',\n",
       " 'brand',\n",
       " 'break',\n",
       " 'bring',\n",
       " 'brother',\n",
       " 'bslvyl',\n",
       " 'bt',\n",
       " 'bucks',\n",
       " 'budget',\n",
       " 'bugis',\n",
       " 'bus',\n",
       " 'busy',\n",
       " 'buy',\n",
       " 'buying',\n",
       " 'bx420',\n",
       " 'cafe',\n",
       " 'cake',\n",
       " 'cal',\n",
       " 'call2optout',\n",
       " 'called',\n",
       " 'caller',\n",
       " 'calling',\n",
       " 'calls',\n",
       " 'camcorder',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'campus',\n",
       " 'cancel',\n",
       " 'cancer',\n",
       " 'car',\n",
       " 'card',\n",
       " 'care',\n",
       " 'carlos',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'catch',\n",
       " 'cause',\n",
       " 'cc',\n",
       " 'cd',\n",
       " 'cell',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'character',\n",
       " 'charge',\n",
       " 'charged',\n",
       " 'chat',\n",
       " 'cheap',\n",
       " 'check',\n",
       " 'checked',\n",
       " 'checking',\n",
       " 'cheers',\n",
       " 'chennai',\n",
       " 'chikku',\n",
       " 'childish',\n",
       " 'children',\n",
       " 'choose',\n",
       " 'christmas',\n",
       " 'cine',\n",
       " 'cinema',\n",
       " 'claim',\n",
       " 'class',\n",
       " 'clean',\n",
       " 'click',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'club',\n",
       " 'code',\n",
       " 'coffee',\n",
       " 'cold',\n",
       " 'colleagues',\n",
       " 'collect',\n",
       " 'collection',\n",
       " 'college',\n",
       " 'colour',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'comin',\n",
       " 'coming',\n",
       " 'comp',\n",
       " 'company',\n",
       " 'competition',\n",
       " 'complete',\n",
       " 'completely',\n",
       " 'complimentary',\n",
       " 'computer',\n",
       " 'confirm',\n",
       " 'congrats',\n",
       " 'congratulations',\n",
       " 'contact',\n",
       " 'content',\n",
       " 'contract',\n",
       " 'convey',\n",
       " 'cool',\n",
       " 'copy',\n",
       " 'correct',\n",
       " 'cos',\n",
       " 'cost',\n",
       " 'costa',\n",
       " 'couldn',\n",
       " 'country',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'cover',\n",
       " 'coz',\n",
       " 'cr9',\n",
       " 'crave',\n",
       " 'crazy',\n",
       " 'credit',\n",
       " 'credits',\n",
       " 'croydon',\n",
       " 'cs',\n",
       " 'cum',\n",
       " 'cup',\n",
       " 'currently',\n",
       " 'custcare',\n",
       " 'customer',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'cuz',\n",
       " 'da',\n",
       " 'dad',\n",
       " 'daddy',\n",
       " 'daily',\n",
       " 'damn',\n",
       " 'darlin',\n",
       " 'darren',\n",
       " 'dat',\n",
       " 'date',\n",
       " 'dating',\n",
       " 'day',\n",
       " 'days',\n",
       " 'dead',\n",
       " 'deal',\n",
       " 'dear',\n",
       " 'decide',\n",
       " 'decided',\n",
       " 'decimal',\n",
       " 'deep',\n",
       " 'definitely',\n",
       " 'del',\n",
       " 'delivery',\n",
       " 'den',\n",
       " 'depends',\n",
       " 'details',\n",
       " 'dey',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'didnt',\n",
       " 'die',\n",
       " 'died',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'digital',\n",
       " 'din',\n",
       " 'dinner',\n",
       " 'dint',\n",
       " 'direct',\n",
       " 'directly',\n",
       " 'dis',\n",
       " 'discount',\n",
       " 'disturb',\n",
       " 'dnt',\n",
       " 'doctor',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'doesnt',\n",
       " 'dog',\n",
       " 'dogging',\n",
       " 'doin',\n",
       " 'doing',\n",
       " 'don',\n",
       " 'dont',\n",
       " 'door',\n",
       " 'double',\n",
       " 'download',\n",
       " 'draw',\n",
       " 'dream',\n",
       " 'dreams',\n",
       " 'drink',\n",
       " 'drive',\n",
       " 'driving',\n",
       " 'drop',\n",
       " 'drugs',\n",
       " 'dude',\n",
       " 'dun',\n",
       " 'dunno',\n",
       " 'dvd',\n",
       " 'earlier',\n",
       " 'early',\n",
       " 'earth',\n",
       " 'easy',\n",
       " 'eat',\n",
       " 'eatin',\n",
       " 'eating',\n",
       " 'eh',\n",
       " 'em',\n",
       " 'email',\n",
       " 'end',\n",
       " 'ended',\n",
       " 'ending',\n",
       " 'ends',\n",
       " 'energy',\n",
       " 'england',\n",
       " 'enjoy',\n",
       " 'enter',\n",
       " 'entered',\n",
       " 'entitled',\n",
       " 'entry',\n",
       " 'er',\n",
       " 'especially',\n",
       " 'eve',\n",
       " 'evening',\n",
       " 'evng',\n",
       " 'ex',\n",
       " 'exactly',\n",
       " 'exam',\n",
       " 'excellent',\n",
       " 'exciting',\n",
       " 'excuse',\n",
       " 'experience',\n",
       " 'expires',\n",
       " 'extra',\n",
       " 'eyes',\n",
       " 'face',\n",
       " 'facebook',\n",
       " 'fact',\n",
       " 'fall',\n",
       " 'family',\n",
       " 'fancy',\n",
       " 'fantasies',\n",
       " 'fantastic',\n",
       " 'far',\n",
       " 'fast',\n",
       " 'fat',\n",
       " 'father',\n",
       " 'fault',\n",
       " 'fb',\n",
       " 'feb',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'feels',\n",
       " 'felt',\n",
       " 'figure',\n",
       " 'film',\n",
       " 'final',\n",
       " 'finally',\n",
       " 'fine',\n",
       " 'fingers',\n",
       " 'finish',\n",
       " 'finished',\n",
       " 'fixed',\n",
       " 'flights',\n",
       " 'flower',\n",
       " 'following',\n",
       " 'fone',\n",
       " 'food',\n",
       " 'forever',\n",
       " 'forget',\n",
       " 'forgot',\n",
       " 'forward',\n",
       " 'forwarded',\n",
       " 'fr',\n",
       " 'free',\n",
       " 'freemsg',\n",
       " 'freephone',\n",
       " 'frens',\n",
       " 'fri',\n",
       " 'friday',\n",
       " 'friend',\n",
       " 'friends',\n",
       " 'friendship',\n",
       " 'frm',\n",
       " 'frnd',\n",
       " 'frnds',\n",
       " 'fuck',\n",
       " 'fucking',\n",
       " 'fullonsms',\n",
       " 'fun',\n",
       " 'funny',\n",
       " 'future',\n",
       " 'fyi',\n",
       " 'gal',\n",
       " 'game',\n",
       " 'games',\n",
       " 'gas',\n",
       " 'gave',\n",
       " 'gay',\n",
       " 'gd',\n",
       " 'ge',\n",
       " 'gee',\n",
       " 'gets',\n",
       " 'gettin',\n",
       " 'getting',\n",
       " 'getzed',\n",
       " 'gift',\n",
       " 'girl',\n",
       " 'girls',\n",
       " 'gives',\n",
       " 'giving',\n",
       " 'glad',\n",
       " 'gn',\n",
       " 'god',\n",
       " 'goes',\n",
       " 'goin',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'gonna',\n",
       " 'good',\n",
       " 'goodmorning',\n",
       " 'goodnight',\n",
       " 'got',\n",
       " 'goto',\n",
       " 'gotta',\n",
       " 'gr8',\n",
       " 'great',\n",
       " 'green',\n",
       " 'grins',\n",
       " 'group',\n",
       " 'gt',\n",
       " 'guaranteed',\n",
       " 'gud',\n",
       " 'guess',\n",
       " 'guy',\n",
       " 'guys',\n",
       " 'gym',\n",
       " 'ha',\n",
       " 'haf',\n",
       " 'haha',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'handset',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'happening',\n",
       " 'happens',\n",
       " 'happiness',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'hav',\n",
       " 'haven',\n",
       " 'havent',\n",
       " 'having',\n",
       " 'head',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heart',\n",
       " 'heavy',\n",
       " 'hee',\n",
       " 'hell',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hey',\n",
       " 'hg',\n",
       " 'hi',\n",
       " 'high',\n",
       " 'hit',\n",
       " 'hiya',\n",
       " 'hl',\n",
       " 'hmm',\n",
       " 'hmmm',\n",
       " 'ho',\n",
       " 'hold',\n",
       " 'holder',\n",
       " 'holding',\n",
       " 'holiday',\n",
       " 'holla',\n",
       " 'home',\n",
       " 'honey',\n",
       " 'hope',\n",
       " 'hoping',\n",
       " 'horny',\n",
       " 'hospital',\n",
       " 'hot',\n",
       " 'hotel',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'hows',\n",
       " 'howz',\n",
       " 'hr',\n",
       " 'hrs',\n",
       " 'http',\n",
       " 'huh',\n",
       " 'hungry',\n",
       " 'hunny',\n",
       " 'hurry',\n",
       " 'hurt',\n",
       " 'hurts',\n",
       " 'hw',\n",
       " 'iam',\n",
       " 'ice',\n",
       " 'id',\n",
       " 'idea',\n",
       " 'identifier',\n",
       " 'il',\n",
       " 'ill',\n",
       " 'im',\n",
       " 'imagine',\n",
       " 'imma',\n",
       " 'immediately',\n",
       " 'important',\n",
       " 'including',\n",
       " 'india',\n",
       " 'indian',\n",
       " 'info',\n",
       " 'information',\n",
       " 'informed',\n",
       " 'inside',\n",
       " 'instead',\n",
       " 'interested',\n",
       " 'internet',\n",
       " 'invited',\n",
       " 'inviting',\n",
       " 'ip4',\n",
       " 'ipod',\n",
       " 'ish',\n",
       " 'isn',\n",
       " 'isnt',\n",
       " 'ive',\n",
       " 'izzit',\n",
       " 'january',\n",
       " 'jay',\n",
       " 'job',\n",
       " 'john',\n",
       " 'join',\n",
       " 'joined',\n",
       " 'joke',\n",
       " 'joking',\n",
       " 'joy',\n",
       " 'jst',\n",
       " 'jus',\n",
       " 'just',\n",
       " 'juz',\n",
       " 'kallis',\n",
       " 'kano',\n",
       " 'kate',\n",
       " 'kb',\n",
       " 'keeping',\n",
       " 'kept',\n",
       " 'kerala',\n",
       " 'kick',\n",
       " 'kids',\n",
       " 'kind',\n",
       " 'kinda',\n",
       " 'king',\n",
       " 'kiss',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'knows',\n",
       " 'knw',\n",
       " 'l8r',\n",
       " 'la',\n",
       " 'ladies',\n",
       " 'lady',\n",
       " 'land',\n",
       " 'landline',\n",
       " 'laptop',\n",
       " 'lar',\n",
       " 'late',\n",
       " 'later',\n",
       " 'latest',\n",
       " 'laugh',\n",
       " 'lazy',\n",
       " 'ldew',\n",
       " 'ldn',\n",
       " 'learn',\n",
       " 'leave',\n",
       " 'leaves',\n",
       " 'leaving',\n",
       " 'lect',\n",
       " 'left',\n",
       " 'leh',\n",
       " 'lei',\n",
       " 'lemme',\n",
       " 'lesson',\n",
       " 'lessons',\n",
       " 'let',\n",
       " 'lets',\n",
       " 'liao',\n",
       " 'library',\n",
       " 'life',\n",
       " 'lift',\n",
       " 'light',\n",
       " 'like',\n",
       " 'liked',\n",
       " 'line',\n",
       " 'link',\n",
       " 'list',\n",
       " 'listen',\n",
       " 'little',\n",
       " 'live',\n",
       " 'll',\n",
       " 'lmao',\n",
       " 'loads',\n",
       " 'loan',\n",
       " 'local',\n",
       " 'log',\n",
       " 'login',\n",
       " 'lol',\n",
       " 'london',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'lookin',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'lor',\n",
       " 'lose',\n",
       " 'lost',\n",
       " 'lot',\n",
       " 'lots',\n",
       " 'lovable',\n",
       " 'love',\n",
       " 'loved',\n",
       " 'lovely',\n",
       " 'lover',\n",
       " 'loverboy',\n",
       " 'loves',\n",
       " 'loving',\n",
       " 'loyalty',\n",
       " 'lt',\n",
       " 'luck',\n",
       " 'lucky',\n",
       " 'lunch',\n",
       " 'luv',\n",
       " 'mah',\n",
       " 'maid',\n",
       " 'mail',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'man',\n",
       " 'march',\n",
       " 'mark',\n",
       " 'marriage',\n",
       " 'married',\n",
       " 'marry',\n",
       " 'match',\n",
       " 'matches',\n",
       " 'mate',\n",
       " 'mates',\n",
       " 'max10mins',\n",
       " 'maximize',\n",
       " 'mayb',\n",
       " 'maybe',\n",
       " 'mean',\n",
       " 'meaning',\n",
       " 'means',\n",
       " 'meant',\n",
       " 'medical',\n",
       " 'meds',\n",
       " 'meet',\n",
       " 'meeting',\n",
       " 'meh',\n",
       " 'member',\n",
       " 'men',\n",
       " 'merry',\n",
       " 'message',\n",
       " 'messages',\n",
       " 'met',\n",
       " 'mid',\n",
       " 'midnight',\n",
       " 'min',\n",
       " 'mind',\n",
       " 'mins',\n",
       " 'minute',\n",
       " 'minutes',\n",
       " 'miracle',\n",
       " 'miss',\n",
       " 'missed',\n",
       " 'missing',\n",
       " 'mistake',\n",
       " 'mm',\n",
       " 'mo',\n",
       " 'moan',\n",
       " 'mob',\n",
       " 'mobile',\n",
       " 'mobiles',\n",
       " 'mobileupd8',\n",
       " 'mode',\n",
       " 'model',\n",
       " 'mom',\n",
       " 'moment',\n",
       " 'mon',\n",
       " 'monday',\n",
       " 'money',\n",
       " 'month',\n",
       " 'months',\n",
       " 'mood',\n",
       " 'moon',\n",
       " 'moral',\n",
       " 'morning',\n",
       " 'mother',\n",
       " 'motorola',\n",
       " 'movie',\n",
       " 'movies',\n",
       " 'mp3',\n",
       " 'mr',\n",
       " 'mrng',\n",
       " 'mrt',\n",
       " 'msg',\n",
       " 'msgs',\n",
       " 'mu',\n",
       " 'mum',\n",
       " 'murder',\n",
       " 'murdered',\n",
       " 'murderer',\n",
       " 'music',\n",
       " 'muz',\n",
       " 'na',\n",
       " 'nah',\n",
       " 'naked',\n",
       " 'national',\n",
       " 'nature',\n",
       " 'naughty',\n",
       " 'near',\n",
       " 'need',\n",
       " 'needs',\n",
       " 'net',\n",
       " 'network',\n",
       " 'neva',\n",
       " 'new',\n",
       " 'news',\n",
       " 'ni8',\n",
       " 'nice',\n",
       " 'nigeria',\n",
       " 'night',\n",
       " 'nights',\n",
       " 'nimya',\n",
       " 'nite',\n",
       " 'no1',\n",
       " 'noe',\n",
       " 'nokia',\n",
       " 'noon',\n",
       " 'nope',\n",
       " 'norm150p',\n",
       " 'normal',\n",
       " 'notice',\n",
       " 'nt',\n",
       " 'ntt',\n",
       " 'number',\n",
       " 'numbers',\n",
       " 'nxt',\n",
       " 'nyt',\n",
       " 'o2',\n",
       " 'offer',\n",
       " 'offers',\n",
       " 'office',\n",
       " 'official',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'okie',\n",
       " 'old',\n",
       " 'omg',\n",
       " 'omw',\n",
       " 'ones',\n",
       " 'online',\n",
       " 'oops',\n",
       " 'open',\n",
       " 'operator',\n",
       " 'opinion',\n",
       " 'opt',\n",
       " 'orange',\n",
       " 'orchard',\n",
       " 'order',\n",
       " 'oredi',\n",
       " 'oso',\n",
       " 'outside',\n",
       " 'pa',\n",
       " 'page',\n",
       " 'paid',\n",
       " 'pain',\n",
       " 'paper',\n",
       " 'parents',\n",
       " 'park',\n",
       " 'partner',\n",
       " 'party',\n",
       " 'pass',\n",
       " 'password',\n",
       " 'past',\n",
       " 'pay',\n",
       " 'paying',\n",
       " 'pc',\n",
       " 'people',\n",
       " 'person',\n",
       " 'personal',\n",
       " 'persons',\n",
       " 'pete',\n",
       " 'phone',\n",
       " 'phones',\n",
       " 'photo',\n",
       " 'pic',\n",
       " 'pick',\n",
       " 'picked',\n",
       " 'picking',\n",
       " 'pics',\n",
       " 'pilates',\n",
       " 'pin',\n",
       " 'pix',\n",
       " 'pizza',\n",
       " 'place',\n",
       " 'plan',\n",
       " 'planned',\n",
       " 'planning',\n",
       " 'plans',\n",
       " 'play',\n",
       " 'player',\n",
       " 'players',\n",
       " 'pleased',\n",
       " 'pleasure',\n",
       " 'pls',\n",
       " 'plus',\n",
       " 'plz',\n",
       " 'pm',\n",
       " 'po',\n",
       " 'pobox',\n",
       " 'pobox334',\n",
       " 'pobox84',\n",
       " 'point',\n",
       " 'points',\n",
       " 'poly',\n",
       " 'polys',\n",
       " 'poor',\n",
       " 'possible',\n",
       " 'post',\n",
       " 'posted',\n",
       " 'pound',\n",
       " 'pounds',\n",
       " 'power',\n",
       " 'pray',\n",
       " 'present',\n",
       " 'press',\n",
       " 'pretty',\n",
       " 'price',\n",
       " 'princess',\n",
       " 'private',\n",
       " 'prize',\n",
       " 'prob',\n",
       " 'probably',\n",
       " 'problem',\n",
       " 'problems',\n",
       " 'process',\n",
       " 'project',\n",
       " 'promise',\n",
       " 'pub',\n",
       " 'pussy',\n",
       " 'putting',\n",
       " 'qatar',\n",
       " 'quality',\n",
       " 'question',\n",
       " 'questions',\n",
       " 'quick',\n",
       " 'quite',\n",
       " 'quiz',\n",
       " 'rain',\n",
       " 'raining',\n",
       " 'rakhesh',\n",
       " 'random',\n",
       " 'rate',\n",
       " 'rates',\n",
       " 'rcvd',\n",
       " 'reach',\n",
       " 'reached',\n",
       " 'reaching',\n",
       " 'read',\n",
       " 'reading',\n",
       " 'ready',\n",
       " 'real',\n",
       " 'really',\n",
       " 'realy',\n",
       " 'reason',\n",
       " 'receive',\n",
       " 'recently',\n",
       " 'red',\n",
       " 'redeemed',\n",
       " 'reference',\n",
       " 'regards',\n",
       " 'registered',\n",
       " 'relax',\n",
       " 'remember',\n",
       " 'remove',\n",
       " 'rent',\n",
       " 'rental',\n",
       " 'replied',\n",
       " 'reply',\n",
       " 'replying',\n",
       " 'representative',\n",
       " 'request',\n",
       " 'rest',\n",
       " 'return',\n",
       " 'reveal',\n",
       " 'review',\n",
       " 'reward',\n",
       " 'right',\n",
       " 'ring',\n",
       " 'ringtone',\n",
       " 'ringtones',\n",
       " 'rite',\n",
       " 'road',\n",
       " 'rock',\n",
       " 'role',\n",
       " 'room',\n",
       " 'rose',\n",
       " 'round',\n",
       " 'row',\n",
       " 'rply',\n",
       " 'rs',\n",
       " 'run',\n",
       " 'sad',\n",
       " 'sae',\n",
       " ...]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learn the vocabulary of ALL messages and save it\n",
    "vect.fit(sms.message)\n",
    "all_tokens = vect.get_feature_names()\n",
    "all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create document-term matrices for ham and spam\n",
    "ham_dtm = vect.transform(sms_ham.message)\n",
    "spam_dtm = vect.transform(sms_spam.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many times EACH token appears across ALL ham messages\n",
    "ham_counts = np.sum(ham_dtm.toarray(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many times EACH token appears across ALL spam messages\n",
    "spam_counts = np.sum(spam_dtm.toarray(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame of tokens with their separate ham and spam counts\n",
    "token_counts = pd.DataFrame({'token':all_tokens, 'ham':ham_counts, 'spam':spam_counts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add one to ham and spam counts to avoid dividing by zero (in the step that follows)\n",
    "token_counts['ham'] = token_counts.ham + 1\n",
    "token_counts['spam'] = token_counts.spam + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "      <th>spam_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>gt</td>\n",
       "      <td>319</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>lt</td>\n",
       "      <td>317</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>lor</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>da</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>later</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>52.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>tone</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>61.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>150p</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>72.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>prize</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>94.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>claim</td>\n",
       "      <td>1</td>\n",
       "      <td>114</td>\n",
       "      <td>114.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1364 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      token  ham  spam  spam_ratio\n",
       "506      gt  319     1    0.003135\n",
       "709      lt  317     1    0.003155\n",
       "695     lor  163     1    0.006135\n",
       "307      da  151     1    0.006623\n",
       "648   later  136     1    0.007353\n",
       "...     ...  ...   ...         ...\n",
       "30       18    1    52   52.000000\n",
       "1193   tone    1    61   61.000000\n",
       "26     150p    1    72   72.000000\n",
       "926   prize    1    94   94.000000\n",
       "246   claim    1   114  114.000000\n",
       "\n",
       "[1364 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate ratio of spam-to-ham for each token\n",
    "token_counts['spam_ratio'] = token_counts.spam / token_counts.ham\n",
    "token_counts.sort_values('spam_ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Building a Naive Bayes model\n",
    "\n",
    "We will use [Multinomial Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html):\n",
    "\n",
    "> The multinomial Naive Bayes classifier is suitable for classification with **discrete features** (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x1279 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 23463 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a Naive Bayes model using X_train_dtm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9834888729361091\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1198   10]\n",
      " [  13  172]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.71879642e-03, 3.90291031e-04, 1.34481933e-01, ...,\n",
       "       1.57609186e-05, 1.00000000e+00, 2.05392162e-07])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict (poorly calibrated) probabilities\n",
    "y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9925496688741722\n"
     ]
    }
   ],
   "source": [
    "# calculate AUC\n",
    "print(metrics.roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4773    Hi, Mobile no.  &lt;#&gt;  has added you in th...\n",
       "4419                           When you get free, call me\n",
       "2340    Cheers for the message Zogtorius. Ive been st...\n",
       "2903    Bill, as in: Are there any letters for me. i’m...\n",
       "45                       No calls..messages..missed calls\n",
       "3589    If you were/are free i can give. Otherwise nal...\n",
       "3120                             Stop knowing me so well!\n",
       "3415                              No pic. Please re-send.\n",
       "1160    Yun buying... But school got offer 2000 plus o...\n",
       "1988                     No calls..messages..missed calls\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print message text for the false positives\n",
    "X_test[y_test < y_pred_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1217    You have 1 new voicemail. Please call 08719181...\n",
       "2295     You have 1 new message. Please call 08718738034.\n",
       "420     Send a logo 2 ur lover - 2 names joined by a h...\n",
       "5110      You have 1 new message. Please call 08715205273\n",
       "3530    Xmas & New Years Eve tickets are now on sale f...\n",
       "1893    CALL 09090900040 & LISTEN TO EXTREME DIRTY LIV...\n",
       "4298    thesmszone.com lets you send free anonymous an...\n",
       "4949    Hi this is Amy, we will be sending you a free ...\n",
       "3991    (Bank of Granite issues Strong-Buy) EXPLOSIVE ...\n",
       "2941     You have 1 new message. Please call 08712400200.\n",
       "2821    INTERFLORA - It's not too late to order Inter...\n",
       "2247    Hi ya babe x u 4goten bout me?' scammers getti...\n",
       "4514    Money i have won wining number 946 wot do i do...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print message text for the false negatives\n",
    "X_test[y_test > y_pred_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FREE MESSAGE Activate your 500 FREE Text Messages by replying to this message with the word FREE For terms & conditions, visit www.07781482378.com'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what do you notice about the false negatives?\n",
    "X_test[3316]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
