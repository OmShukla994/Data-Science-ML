{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining using Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is NLP?\n",
    "\n",
    "- Using computers to process (analyze, understand, generate) natural human languages\n",
    "- Most knowledge created by humans is unstructured text, and we need a way to make sense of it\n",
    "- Build probabilistic model using data about a language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Packages Related to Textmining\n",
    "- **textmining1.0:** contains a variety of useful functions for text mining in Python.\n",
    "- **NLTK:** This package can be extremely useful because you have easy access to over 50 corpora and lexical resources\n",
    "- **Tweepy:** to mine Twitter data\n",
    "- **scrappy:**  extract the data you need from websites\n",
    "- **urllib2:** a package for opening URLs\n",
    "- **requests:** library for grabbing data from the internet\n",
    "- **Beautifulsoup:** library for parsing HTML data\n",
    "- **re:**  grep(), grepl(), regexpr(), gregexpr(), sub(), gsub(), and strsplit() are helpful functions\n",
    "- **wordcloud:** to visualize the wordcloud\n",
    "- **Textblob:** to used for text processing (nlp- lowel events)\n",
    "- **sklearn:** to used for preprocessing, modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some of the higher level task areas?\n",
    "\n",
    "- **Information retrieval**: Find relevant results and similar results\n",
    "    - [Google](https://www.google.com/)\n",
    "- **Information extraction**: Structured information from unstructured documents\n",
    "    - [Events from Gmail](https://support.google.com/calendar/answer/6084018?hl=en)\n",
    "- **Machine translation**: One language to another\n",
    "    - [Google Translate](https://translate.google.com/)\n",
    "- **Text simplification**: Preserve the meaning of text, but simplify the grammar and vocabulary\n",
    "    - [Rewordify](https://rewordify.com/)\n",
    "    - [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page)\n",
    "- **Predictive text input**: Faster or easier typing\n",
    "    - [My application](https://justmarkham.shinyapps.io/textprediction/)\n",
    "    - [A much better application](https://farsite.shinyapps.io/swiftkey-cap/)\n",
    "- **Sentiment analysis**: Attitude of speaker\n",
    "    - [Hater News](http://haternews.herokuapp.com/)\n",
    "- **Automatic summarization**: Extractive or abstractive summarization\n",
    "    - [autotldr](https://www.reddit.com/r/technology/comments/35brc8/21_million_people_still_use_aol_dialup/cr2zzj0)\n",
    "- **Natural Language Generation**: Generate text from data\n",
    "    - [How a computer describes a sports match](http://www.bbc.com/news/technology-34204052)\n",
    "    - [Publishers withdraw more than 120 gibberish papers](http://www.nature.com/news/publishers-withdraw-more-than-120-gibberish-papers-1.14763)\n",
    "- **Speech recognition and generation**: Speech-to-text, text-to-speech\n",
    "    - [Google's Web Speech API demo](https://www.google.com/intl/en/chrome/demos/speech.html)\n",
    "    - [Vocalware Text-to-Speech demo](https://www.vocalware.com/index/demo)\n",
    "- **Question answering**: Determine the intent of the question, match query with knowledge base, evaluate hypotheses\n",
    "    - [How did supercomputer Watson beat Jeopardy champion Ken Jennings?](http://blog.ted.com/how-did-supercomputer-watson-beat-jeopardy-champion-ken-jennings-experts-discuss/)\n",
    "    - [IBM's Watson Trivia Challenge](http://www.nytimes.com/interactive/2010/06/16/magazine/watson-trivia-game.html)\n",
    "    - [The AI Behind Watson](http://www.aaai.org/Magazine/Watson/watson.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing - What are some of the lower level components?\n",
    "\n",
    "- **Tokenization**: breaking text into tokens (words, sentences, n-grams)\n",
    "- **Stopword removal**: a/an/the\n",
    "- **Stemming and lemmatization**: root word\n",
    "- **TF-IDF**: word importance\n",
    "- **Part-of-speech tagging**: noun/verb/adjective\n",
    "- **Named entity recognition**: person/organization/location\n",
    "- **Spelling correction**: \"New Yrok City\"\n",
    "- **Word sense disambiguation**: \"buy a mouse\"\n",
    "- **Segmentation**: \"New York City subway\"\n",
    "- **Language detection**: \"translate this page\"\n",
    "- **Machine learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is NLP hard?\n",
    "\n",
    "- **Ambiguity**:\n",
    "    - Hospitals are Sued by 7 Foot Doctors\n",
    "    - Juvenile Court to Try Shooting Defendant\n",
    "    - Local High School Dropouts Cut in Half\n",
    "- **Non-standard English**: text messages\n",
    "- **Idioms**: \"throw in the towel\"\n",
    "- **Newly coined words**: \"retweet\"\n",
    "- **Tricky entity names**: \"Where is A Bug's Life playing?\"\n",
    "- **World knowledge**: \"Mary and Sue are sisters\", \"Mary and Sue are mothers\"\n",
    "\n",
    "NLP requires an understanding of the **language** and the **world**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "##### TF-IDF Vectors as features\n",
    "- TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "- IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "- TF-IDF Vectors can be generated at different levels of input tokens (words, characters, n-grams)\n",
    "    - a. Word Level TF-IDF : Matrix representing tf-idf scores of every term in different documents\n",
    "    - b. N-gram Level TF-IDF : N-grams are the combination of N terms together. This Matrix representing tf-idf scores of N-grams\n",
    "    - c. Character Level TF-IDF : Matrix representing tf-idf scores of character level n-grams in the corpus\n",
    "\n",
    "##### Text / NLP based features\n",
    "- Word Count of the documents – total number of words in the documents\n",
    "- Character Count of the documents – total number of characters in the documents\n",
    "- Average Word Density of the documents – average length of the words used in the documents\n",
    "- Puncutation Count in the Complete Essay – total number of punctuation marks in the documents\n",
    "- Upper Case Count in the Complete Essay – total number of upper count words in the documents\n",
    "- Title Word Count in the Complete Essay – total number of proper case (title) words in the documents\n",
    "- Frequency distribution of Part of Speech Tags:\n",
    "    - Noun Count\n",
    "    - Verb Count\n",
    "    - Adjective Count\n",
    "    - Adverb Count\n",
    "    - ronoun Count\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "- Naive Bayes Classifier\n",
    "- Linear Classifier\n",
    "- Support Vector Machine\n",
    "- KNN\n",
    "- Bagging Models\n",
    "- Boosting Models\n",
    "- Shallow Neural Networks\n",
    "- Deep Neural Networks\n",
    "    - Convolutional Neural Network (CNN)\n",
    "    - Long Short Term Modelr (LSTM)\n",
    "    - Gated Recurrent Unit (GRU)\n",
    "    - Bidirectional RNN\n",
    "    - Recurrent Convolutional Neural Network (RCNN)\n",
    "    - Other Variants of Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Reading in the Yelp Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"corpus\" = collection of documents\n",
    "- \"corpora\" = plural form of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import required packages\n",
    "#basics\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "#misc\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "#stats\n",
    "#from scipy.misc import imread\n",
    "from scipy import sparse\n",
    "import scipy.stats as ss\n",
    "\n",
    "#viz\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud ,STOPWORDS  #conda install -c conda-forge wordcloud\n",
    "from PIL import Image\n",
    "#import matplotlib_venn as venn\n",
    "\n",
    "#nlp\n",
    "import string\n",
    "import re    #for regex\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#import spacy\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tweet tokenizer does not split at apostophes which is what we want\n",
    "from nltk.tokenize import TweetTokenizer   \n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler  #from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "#FeatureEngineering\n",
    "#!pip install lightgbm\n",
    "#from lightgbm import LGBMClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, decomposition, ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import  textblob\n",
    "#import xgboost\n",
    "#from keras.preprocessing import text, sequence\n",
    "#from keras import layers, models, optimizers\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "from textblob import Word\n",
    "\n",
    "#settings\n",
    "start_time=time.time()\n",
    "color = sns.color_palette()\n",
    "sns.set_style(\"dark\")\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "tokenizer=TweetTokenizer()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read yelp.csv into a DataFrame\n",
    "yelp = pd.read_csv('C:/Users/om/Downloads/Text mining & NLP files/Text Mining case study - Yelp Reveiws/reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 10 columns):\n",
      "business_id    10000 non-null object\n",
      "date           10000 non-null object\n",
      "review_id      10000 non-null object\n",
      "stars          10000 non-null int64\n",
      "text           10000 non-null object\n",
      "type           10000 non-null object\n",
      "user_id        10000 non-null object\n",
      "cool           10000 non-null int64\n",
      "useful         10000 non-null int64\n",
      "funny          10000 non-null int64\n",
      "dtypes: int64(4), object(6)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "yelp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp=yelp[['review_id', 'stars', 'text', 'cool', 'useful', 'funny']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id  stars  \\\n",
       "0  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text  cool  useful  funny  \n",
       "0  My wife took me here on my birthday for breakf...     2       5      0  \n",
       "1  I have no idea why some people give bad review...     0       0      0  \n",
       "2  love the gyro plate. Rice is so good and I als...     0       1      0  \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...     1       2      0  \n",
       "4  General Manager Scott Petello is a good egg!!!...     0       0      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = yelp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: It may take some time to process the function if the data is huge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].astype(str)\n",
    "df['count_sent']=df[\"text\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n",
    "\n",
    "#Word count in each comment:\n",
    "df['count_word']=df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "#Unique word count\n",
    "df['count_unique_word']=df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "#Letter count\n",
    "df['count_letters']=df[\"text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "#Word density\n",
    "\n",
    "df['word_density'] = df['count_letters'] / (df['count_word']+1)\n",
    "\n",
    "#punctuation count\n",
    "df[\"count_punctuations\"] =df[\"text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "#upper case words count\n",
    "df[\"count_words_upper\"] = df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "#upper case words count\n",
    "df[\"count_words_lower\"] = df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.islower()]))\n",
    "\n",
    "#title case words count\n",
    "df[\"count_words_title\"] = df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "#Number of stopwords\n",
    "df[\"count_stopwords\"] = df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "#Average length of the words\n",
    "df[\"mean_word_len\"] = df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "#Number of numeric\n",
    "df['numeric'] = df['text'].apply(lambda x :len([x for x in x.split() if x.isdigit()]))\n",
    "\n",
    "#Number of alphanumeric\n",
    "df['alphanumeric'] = df['text'].apply(lambda x :len([x for x in x.split() if x.isalnum()]))\n",
    "\n",
    "#Number of alphabetics\n",
    "df['alphabetetics'] = df['text'].apply(lambda x :len([x for x in x.split() if x.isalpha()]))\n",
    "\n",
    "#Number of alphabetics\n",
    "df['Spaces'] = df['text'].apply(lambda x :len([x for x in x.split() if x.isspace()]))\n",
    "\n",
    "#Number of Words ends with\n",
    "df['words_ends_with_et'] = df['text'].apply(lambda x :len([x for x in x.lower().split() if x.endswith('et')]))\n",
    "\n",
    "#Number of Words ends with\n",
    "df['words_start_with_no'] = df['text'].apply(lambda x :len([x for x in x.lower().split() if x.startswith('no')]))\n",
    "\n",
    "# Count the occurences of all words\n",
    "df['wordcounts'] = df['text'].apply(lambda x :dict([ [t, x.split().count(t)] for t in set(x.split()) ]))\n",
    "\n",
    "pos_family = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt\n",
    "\n",
    "df['noun_count'] = df['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "df['verb_count'] = df['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "df['adj_count']  = df['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "df['adv_count']  = df['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "df['pron_count'] = df['text'].apply(lambda x: check_pos_tag(x, 'pron')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Sentiment analysis using Textblob module"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TextBlob returns polarity and subjectivity of a sentence. Polarity lies between [-1,1], -1 defines a negative sentiment and 1 defines a positive sentiment.\n",
    "Subjectivity quantifies the amount of personal opinion and factual information contained in the text. The higher subjectivity means that the text contains personal opinion rather than factual information."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For example: We calculated polarity and subjectivity for “I do not like this example at all, it is too boring”. For this particular example, polarity = -1 and subjectivity is 1, which is fair.\n",
    "\n",
    "However, for the sentence “This was a helpful example but I would prefer another one”. It returns 0.0 for both subjectivity and polarity which is not the finest answer we’d expect.\n",
    "\n",
    "#Example for tweet\n",
    "--------------------\n",
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment_analysis(tweet):\n",
    " def getSubjectivity(text):\n",
    "   return TextBlob(text).sentiment.subjectivity\n",
    "  \n",
    " #Create a function to get the polarity\n",
    " def getPolarity(text):\n",
    "   return TextBlob(text).sentiment.polarity\n",
    "  \n",
    " #Create two new columns ‘Subjectivity’ & ‘Polarity’\n",
    " tweet[‘TextBlob_Subjectivity’] =    tweet[‘tweet’].apply(getSubjectivity)\n",
    " tweet [‘TextBlob_Polarity’] = tweet[‘tweet’].apply(getPolarity)\n",
    " def getAnalysis(score):\n",
    "  if score < 0:\n",
    "    return ‘Negative’\n",
    "  elif score == 0:\n",
    "    return ‘Neutral’\n",
    "  else:\n",
    "    return ‘Positive’\n",
    " tweet [‘TextBlob_Analysis’] = tweet  [‘TextBlob_Polarity’].apply(getAnalysis )\n",
    "return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df[\"text\"].apply(lambda x: TextBlob(x).sentiment.polarity )\n",
    "#check +ve and -ve word/sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    3526\n",
       "5    3337\n",
       "3    1461\n",
       "2     927\n",
       "1     749\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.stars.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7500,)\n",
      "(2500,)\n",
      "(7500,)\n",
      "(2500,)\n"
     ]
    }
   ],
   "source": [
    "# create a new DataFrame that only contains the 5-star and 1-star reviews\n",
    "#yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "\n",
    "# define X and y\n",
    "X = yelp.text\n",
    "y = yelp.stars\n",
    "\n",
    "# split the new DataFrame into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 30)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>count_word</th>\n",
       "      <th>count_unique_word</th>\n",
       "      <th>count_letters</th>\n",
       "      <th>...</th>\n",
       "      <th>Spaces</th>\n",
       "      <th>words_ends_with_et</th>\n",
       "      <th>words_start_with_no</th>\n",
       "      <th>wordcounts</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>pron_count</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>155</td>\n",
       "      <td>110</td>\n",
       "      <td>889</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>{'like': 1, 'scrambled': 1, 'get': 2, 'Our': 1...</td>\n",
       "      <td>26</td>\n",
       "      <td>35</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>0.402469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>257</td>\n",
       "      <td>159</td>\n",
       "      <td>1345</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'because': 1, 'why': 1, 'what': 1, '\"Here's':...</td>\n",
       "      <td>48</td>\n",
       "      <td>53</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>35</td>\n",
       "      <td>0.229773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Rice': 1, 'dig': 1, 'the': 1, 'also': 1, 'is...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>76</td>\n",
       "      <td>61</td>\n",
       "      <td>419</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>{'ballparks,': 1, 'Dog': 1, 'a': 4, 'The': 2, ...</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.608646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>86</td>\n",
       "      <td>72</td>\n",
       "      <td>469</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'life!!': 1, 'Thanks': 1, 'if': 2, 'a': 2, 'g...</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>0.468125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id  stars  \\\n",
       "0  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text  cool  useful  funny  \\\n",
       "0  My wife took me here on my birthday for breakf...     2       5      0   \n",
       "1  I have no idea why some people give bad review...     0       0      0   \n",
       "2  love the gyro plate. Rice is so good and I als...     0       1      0   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...     1       2      0   \n",
       "4  General Manager Scott Petello is a good egg!!!...     0       0      0   \n",
       "\n",
       "   count_sent  count_word  count_unique_word  count_letters  ...  Spaces  \\\n",
       "0           7         155                110            889  ...       0   \n",
       "1           5         257                159           1345  ...       0   \n",
       "2           1          16                 16             76  ...       0   \n",
       "3           5          76                 61            419  ...       0   \n",
       "4           3          86                 72            469  ...       0   \n",
       "\n",
       "   words_ends_with_et  words_start_with_no  \\\n",
       "0                   3                    0   \n",
       "1                   1                    2   \n",
       "2                   0                    0   \n",
       "3                   1                    0   \n",
       "4                   1                    1   \n",
       "\n",
       "                                          wordcounts  noun_count  verb_count  \\\n",
       "0  {'like': 1, 'scrambled': 1, 'get': 2, 'Our': 1...          26          35   \n",
       "1  {'because': 1, 'why': 1, 'what': 1, '\"Here's':...          48          53   \n",
       "2  {'Rice': 1, 'dig': 1, 'the': 1, 'also': 1, 'is...           5           3   \n",
       "3  {'ballparks,': 1, 'Dog': 1, 'a': 4, 'The': 2, ...          28          12   \n",
       "4  {'life!!': 1, 'Thanks': 1, 'if': 2, 'a': 2, 'g...          21          19   \n",
       "\n",
       "   adj_count  adv_count  pron_count  sentiment  \n",
       "0         19         14          28   0.402469  \n",
       "1         19         18          35   0.229773  \n",
       "2          1          2           2   0.566667  \n",
       "3          6          1           3   0.608646  \n",
       "4          5          6          14   0.468125  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating user defined functions for clean the text and pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abbrevations and Words correction\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:{}`+=~|.!?,'0-9]\", \"\", text)\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = set(nltk.corpus.stopwords.words('english'))\n",
    "nltk.corpus.stopwords.words('english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concept of NLTK\n",
    "nltk.corpus.stopwords.words('english') #Stop word of English. Nltk work on 50 language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concept of NLTK\n",
    "nltk.corpus.stopwords.words('french') #stop  word of frence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def pre_process(text):\n",
    "    #text = text.str.replace('/','')                           #Replacing the / with none\n",
    "    #text = text.apply(lambda x: re.sub(\"  \",\" \", x))          #Replacing double space with single space\n",
    "    #text = re.sub(r\"[-()\\\"#/@;:{}`+=~|.!?,']\", \"\", text)      #Replacing special character with none\n",
    "    #text = re.sub(r'[0-9]+', '', text)                        #Replacing numbers with none\n",
    "    #text = text.apply(lambda x: \" \".join(x.translate(str.maketrans('', '', string.punctuation)) for x in x.split() if x.isalpha()))\n",
    "    text = text.apply(lambda x: \" \".join(x for x in x.split() if x not in stop)) #Removing stop words\n",
    "    #text = text.apply(lambda x: str(TextBlob(x).correct()))                      #Correct spelling corrections\n",
    "    #text = text.apply(lambda x: \" \".join(PorterStemmer().stem(word) for word in x.split())) #Stemming using porter stemmer\n",
    "    #text = text.apply(lambda x: \" \".join(stemmer_func(word) for word in x.split()))        #Stemming\n",
    "    #text = text.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))   #lemmatization\n",
    "    #text = text.apply(lambda x: \" \".join(word for word, pos in pos_tag(x.split()) if pos not in ['NN','NNS','NNP','NNPS'])) #Removing nouns etc\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(lambda x: clean_text(x))\n",
    "X_test = X_test.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pre_process(X_train)\n",
    "X_test=pre_process(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Vectorization (Count, Tfidf, Hashing)\n",
    "        - Charter level\n",
    "        - Word level\n",
    "        - n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                             ngram_range=(1, 1), max_df = 0.9,\n",
    "                             min_df = 0.01, \n",
    "                             encoding='latin-1') #max_features=800\n",
    "                             \n",
    "xtrain_count = count_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7500x1036 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 283123 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the document term metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm=xtrain_count.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 2, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able',\n",
       " 'absolutely',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addition',\n",
       " 'admit',\n",
       " 'afternoon',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'ahead',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'amazing',\n",
       " 'ambiance',\n",
       " 'american',\n",
       " 'amount',\n",
       " 'another',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'apparently',\n",
       " 'appetizer',\n",
       " 'appetizers',\n",
       " 'appreciate',\n",
       " 'area',\n",
       " 'arent',\n",
       " 'arizona',\n",
       " 'around',\n",
       " 'arrived',\n",
       " 'art',\n",
       " 'asian',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'ate',\n",
       " 'atmosphere',\n",
       " 'attention',\n",
       " 'attentive',\n",
       " 'authentic',\n",
       " 'available',\n",
       " 'average',\n",
       " 'avoid',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'az',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'bacon',\n",
       " 'bad',\n",
       " 'bag',\n",
       " 'baked',\n",
       " 'bar',\n",
       " 'bars',\n",
       " 'bartender',\n",
       " 'bartenders',\n",
       " 'based',\n",
       " 'basically',\n",
       " 'basil',\n",
       " 'bathroom',\n",
       " 'bbq',\n",
       " 'bean',\n",
       " 'beans',\n",
       " 'beat',\n",
       " 'beautiful',\n",
       " 'bed',\n",
       " 'beef',\n",
       " 'beer',\n",
       " 'beers',\n",
       " 'behind',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'better',\n",
       " 'big',\n",
       " 'bill',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'bite',\n",
       " 'black',\n",
       " 'bland',\n",
       " 'blue',\n",
       " 'book',\n",
       " 'bottle',\n",
       " 'bottom',\n",
       " 'bought',\n",
       " 'bowl',\n",
       " 'box',\n",
       " 'boyfriend',\n",
       " 'bread',\n",
       " 'break',\n",
       " 'breakfast',\n",
       " 'bring',\n",
       " 'brought',\n",
       " 'brunch',\n",
       " 'bucks',\n",
       " 'buffet',\n",
       " 'building',\n",
       " 'bun',\n",
       " 'bunch',\n",
       " 'burger',\n",
       " 'burgers',\n",
       " 'burrito',\n",
       " 'business',\n",
       " 'busy',\n",
       " 'butter',\n",
       " 'buy',\n",
       " 'cafe',\n",
       " 'cake',\n",
       " 'call',\n",
       " 'called',\n",
       " 'came',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'car',\n",
       " 'card',\n",
       " 'care',\n",
       " 'carne',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'casual',\n",
       " 'center',\n",
       " 'central',\n",
       " 'certainly',\n",
       " 'chain',\n",
       " 'chairs',\n",
       " 'chance',\n",
       " 'chandler',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'charge',\n",
       " 'cheap',\n",
       " 'cheaper',\n",
       " 'check',\n",
       " 'checked',\n",
       " 'checking',\n",
       " 'cheese',\n",
       " 'chef',\n",
       " 'chicken',\n",
       " 'chili',\n",
       " 'chinese',\n",
       " 'chips',\n",
       " 'chocolate',\n",
       " 'choice',\n",
       " 'choices',\n",
       " 'choose',\n",
       " 'chose',\n",
       " 'city',\n",
       " 'classic',\n",
       " 'clean',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'club',\n",
       " 'coffee',\n",
       " 'cold',\n",
       " 'combination',\n",
       " 'combo',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'comfortable',\n",
       " 'coming',\n",
       " 'company',\n",
       " 'complaint',\n",
       " 'complete',\n",
       " 'completely',\n",
       " 'considering',\n",
       " 'conversation',\n",
       " 'cook',\n",
       " 'cooked',\n",
       " 'cool',\n",
       " 'corn',\n",
       " 'corner',\n",
       " 'cost',\n",
       " 'could',\n",
       " 'couldnt',\n",
       " 'counter',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'cozy',\n",
       " 'crab',\n",
       " 'craving',\n",
       " 'crazy',\n",
       " 'cream',\n",
       " 'creamy',\n",
       " 'crisp',\n",
       " 'crispy',\n",
       " 'crowd',\n",
       " 'crowded',\n",
       " 'crust',\n",
       " 'cup',\n",
       " 'curry',\n",
       " 'customer',\n",
       " 'customers',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'daily',\n",
       " 'damn',\n",
       " 'dark',\n",
       " 'date',\n",
       " 'daughter',\n",
       " 'day',\n",
       " 'days',\n",
       " 'deal',\n",
       " 'decent',\n",
       " 'decided',\n",
       " 'decor',\n",
       " 'deep',\n",
       " 'definitely',\n",
       " 'delicious',\n",
       " 'delish',\n",
       " 'desert',\n",
       " 'dessert',\n",
       " 'desserts',\n",
       " 'didnt',\n",
       " 'die',\n",
       " 'different',\n",
       " 'dining',\n",
       " 'dinner',\n",
       " 'dip',\n",
       " 'dirty',\n",
       " 'disappointed',\n",
       " 'dish',\n",
       " 'dishes',\n",
       " 'doesnt',\n",
       " 'dog',\n",
       " 'dogs',\n",
       " 'done',\n",
       " 'dont',\n",
       " 'door',\n",
       " 'downtown',\n",
       " 'dressing',\n",
       " 'drink',\n",
       " 'drinking',\n",
       " 'drinks',\n",
       " 'drive',\n",
       " 'driving',\n",
       " 'dry',\n",
       " 'due',\n",
       " 'early',\n",
       " 'easily',\n",
       " 'east',\n",
       " 'easy',\n",
       " 'eat',\n",
       " 'eaten',\n",
       " 'eating',\n",
       " 'egg',\n",
       " 'eggs',\n",
       " 'either',\n",
       " 'else',\n",
       " 'employees',\n",
       " 'empty',\n",
       " 'end',\n",
       " 'ended',\n",
       " 'enjoy',\n",
       " 'enjoyed',\n",
       " 'enough',\n",
       " 'entire',\n",
       " 'entree',\n",
       " 'entrees',\n",
       " 'environment',\n",
       " 'especially',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'evening',\n",
       " 'event',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'exactly',\n",
       " 'excellent',\n",
       " 'except',\n",
       " 'excited',\n",
       " 'expect',\n",
       " 'expectations',\n",
       " 'expected',\n",
       " 'expecting',\n",
       " 'expensive',\n",
       " 'experience',\n",
       " 'extra',\n",
       " 'extremely',\n",
       " 'eye',\n",
       " 'fabulous',\n",
       " 'face',\n",
       " 'fact',\n",
       " 'fair',\n",
       " 'fairly',\n",
       " 'family',\n",
       " 'fan',\n",
       " 'fancy',\n",
       " 'fantastic',\n",
       " 'far',\n",
       " 'fast',\n",
       " 'fat',\n",
       " 'favorite',\n",
       " 'favorites',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'felt',\n",
       " 'filled',\n",
       " 'filling',\n",
       " 'finally',\n",
       " 'find',\n",
       " 'fine',\n",
       " 'finish',\n",
       " 'finished',\n",
       " 'first',\n",
       " 'fish',\n",
       " 'fit',\n",
       " 'five',\n",
       " 'fix',\n",
       " 'flat',\n",
       " 'flavor',\n",
       " 'flavorful',\n",
       " 'flavors',\n",
       " 'floor',\n",
       " 'folks',\n",
       " 'food',\n",
       " 'foods',\n",
       " 'forever',\n",
       " 'forget',\n",
       " 'forward',\n",
       " 'found',\n",
       " 'four',\n",
       " 'free',\n",
       " 'french',\n",
       " 'fresh',\n",
       " 'friday',\n",
       " 'fried',\n",
       " 'friend',\n",
       " 'friendly',\n",
       " 'friends',\n",
       " 'fries',\n",
       " 'front',\n",
       " 'frozen',\n",
       " 'fruit',\n",
       " 'full',\n",
       " 'fun',\n",
       " 'future',\n",
       " 'game',\n",
       " 'garlic',\n",
       " 'gave',\n",
       " 'generally',\n",
       " 'generous',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'girl',\n",
       " 'girls',\n",
       " 'give',\n",
       " 'given',\n",
       " 'giving',\n",
       " 'glad',\n",
       " 'glass',\n",
       " 'glasses',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'grab',\n",
       " 'greasy',\n",
       " 'great',\n",
       " 'green',\n",
       " 'greeted',\n",
       " 'grill',\n",
       " 'grilled',\n",
       " 'grocery',\n",
       " 'group',\n",
       " 'guacamole',\n",
       " 'guess',\n",
       " 'guy',\n",
       " 'guys',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'hands',\n",
       " 'hang',\n",
       " 'happened',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'hate',\n",
       " 'havent',\n",
       " 'head',\n",
       " 'healthy',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heart',\n",
       " 'heat',\n",
       " 'heaven',\n",
       " 'heavy',\n",
       " 'hell',\n",
       " 'help',\n",
       " 'helped',\n",
       " 'helpful',\n",
       " 'high',\n",
       " 'highly',\n",
       " 'hit',\n",
       " 'home',\n",
       " 'homemade',\n",
       " 'honestly',\n",
       " 'honey',\n",
       " 'hope',\n",
       " 'horrible',\n",
       " 'hostess',\n",
       " 'hot',\n",
       " 'hotel',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'however',\n",
       " 'huge',\n",
       " 'hungry',\n",
       " 'husband',\n",
       " 'ice',\n",
       " 'iced',\n",
       " 'id',\n",
       " 'idea',\n",
       " 'ill',\n",
       " 'im',\n",
       " 'imagine',\n",
       " 'immediately',\n",
       " 'impressed',\n",
       " 'included',\n",
       " 'including',\n",
       " 'ingredients',\n",
       " 'inside',\n",
       " 'instead',\n",
       " 'interesting',\n",
       " 'interior',\n",
       " 'isnt',\n",
       " 'issue',\n",
       " 'italian',\n",
       " 'item',\n",
       " 'items',\n",
       " 'ive',\n",
       " 'job',\n",
       " 'joint',\n",
       " 'keep',\n",
       " 'kept',\n",
       " 'kick',\n",
       " 'kids',\n",
       " 'kind',\n",
       " 'kinda',\n",
       " 'kitchen',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'la',\n",
       " 'lack',\n",
       " 'lady',\n",
       " 'large',\n",
       " 'last',\n",
       " 'late',\n",
       " 'later',\n",
       " 'least',\n",
       " 'leave',\n",
       " 'leaving',\n",
       " 'left',\n",
       " 'lemon',\n",
       " 'less',\n",
       " 'let',\n",
       " 'lets',\n",
       " 'lettuce',\n",
       " 'level',\n",
       " 'life',\n",
       " 'light',\n",
       " 'like',\n",
       " 'liked',\n",
       " 'limited',\n",
       " 'line',\n",
       " 'list',\n",
       " 'literally',\n",
       " 'little',\n",
       " 'live',\n",
       " 'lived',\n",
       " 'living',\n",
       " 'local',\n",
       " 'located',\n",
       " 'location',\n",
       " 'locations',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'looked',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'lost',\n",
       " 'lot',\n",
       " 'lots',\n",
       " 'loud',\n",
       " 'love',\n",
       " 'loved',\n",
       " 'lovely',\n",
       " 'loves',\n",
       " 'low',\n",
       " 'lucky',\n",
       " 'lunch',\n",
       " 'mac',\n",
       " 'made',\n",
       " 'main',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'mall',\n",
       " 'man',\n",
       " 'manager',\n",
       " 'many',\n",
       " 'margaritas',\n",
       " 'market',\n",
       " 'matter',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'meal',\n",
       " 'meals',\n",
       " 'mean',\n",
       " 'means',\n",
       " 'meat',\n",
       " 'meats',\n",
       " 'mediocre',\n",
       " 'medium',\n",
       " 'meet',\n",
       " 'mention',\n",
       " 'menu',\n",
       " 'met',\n",
       " 'mexican',\n",
       " 'middle',\n",
       " 'might',\n",
       " 'mind',\n",
       " 'mine',\n",
       " 'minute',\n",
       " 'minutes',\n",
       " 'miss',\n",
       " 'mix',\n",
       " 'mixed',\n",
       " 'modern',\n",
       " 'mom',\n",
       " 'money',\n",
       " 'month',\n",
       " 'months',\n",
       " 'mood',\n",
       " 'morning',\n",
       " 'mostly',\n",
       " 'mouth',\n",
       " 'move',\n",
       " 'moved',\n",
       " 'movie',\n",
       " 'much',\n",
       " 'mushrooms',\n",
       " 'music',\n",
       " 'must',\n",
       " 'n',\n",
       " 'name',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'needs',\n",
       " 'neighborhood',\n",
       " 'never',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nice',\n",
       " 'nicely',\n",
       " 'night',\n",
       " 'nights',\n",
       " 'none',\n",
       " 'noodles',\n",
       " 'normal',\n",
       " 'normally',\n",
       " 'north',\n",
       " 'note',\n",
       " 'nothing',\n",
       " 'noticed',\n",
       " 'number',\n",
       " 'obviously',\n",
       " 'offer',\n",
       " 'offered',\n",
       " 'offers',\n",
       " 'office',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'oil',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'olive',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'onion',\n",
       " 'onions',\n",
       " 'open',\n",
       " 'opened',\n",
       " 'opening',\n",
       " 'opinion',\n",
       " 'option',\n",
       " 'options',\n",
       " 'orange',\n",
       " 'order',\n",
       " 'ordered',\n",
       " 'ordering',\n",
       " 'orders',\n",
       " 'original',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'outdoor',\n",
       " 'outside',\n",
       " 'outstanding',\n",
       " 'overall',\n",
       " 'overpriced',\n",
       " 'owner',\n",
       " 'owners',\n",
       " 'packed',\n",
       " 'paid',\n",
       " 'paper',\n",
       " 'park',\n",
       " 'parking',\n",
       " 'part',\n",
       " 'particular',\n",
       " 'party',\n",
       " 'pass',\n",
       " 'past',\n",
       " 'pasta',\n",
       " 'patio',\n",
       " 'patrons',\n",
       " 'pay',\n",
       " 'paying',\n",
       " 'people',\n",
       " 'pepper',\n",
       " 'peppers',\n",
       " 'per',\n",
       " 'perfect',\n",
       " 'perfectly',\n",
       " 'perhaps',\n",
       " 'person',\n",
       " 'personal',\n",
       " 'personally',\n",
       " 'phoenix',\n",
       " 'phone',\n",
       " 'pick',\n",
       " 'picked',\n",
       " 'pie',\n",
       " 'piece',\n",
       " 'pieces',\n",
       " 'pita',\n",
       " 'pizza',\n",
       " 'pizzas',\n",
       " 'place',\n",
       " 'places',\n",
       " 'plan',\n",
       " 'plate',\n",
       " 'plates',\n",
       " 'play',\n",
       " 'playing',\n",
       " 'pleasant',\n",
       " 'please',\n",
       " 'plenty',\n",
       " 'plus',\n",
       " 'pm',\n",
       " 'point',\n",
       " 'pool',\n",
       " 'poor',\n",
       " 'pork',\n",
       " 'portion',\n",
       " 'portions',\n",
       " 'potato',\n",
       " 'potatoes',\n",
       " 'prefer',\n",
       " 'prepared',\n",
       " 'pretty',\n",
       " 'previous',\n",
       " 'price',\n",
       " 'priced',\n",
       " 'prices',\n",
       " 'pricey',\n",
       " 'probably',\n",
       " 'problem',\n",
       " 'products',\n",
       " 'professional',\n",
       " 'pulled',\n",
       " 'put',\n",
       " 'quality',\n",
       " 'quick',\n",
       " 'quickly',\n",
       " 'quiet',\n",
       " 'quite',\n",
       " 'rare',\n",
       " 'rather',\n",
       " 'rating',\n",
       " 'read',\n",
       " 'reading',\n",
       " 'ready',\n",
       " 'real',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'reasonable',\n",
       " 'reasonably',\n",
       " 'received',\n",
       " 'recently',\n",
       " 'recommend',\n",
       " 'recommended',\n",
       " 'red',\n",
       " 'regular',\n",
       " 'remember',\n",
       " 'rest',\n",
       " 'restaurant',\n",
       " 'restaurants',\n",
       " 'return',\n",
       " 'review',\n",
       " 'reviews',\n",
       " 'ribs',\n",
       " 'rice',\n",
       " 'rich',\n",
       " 'right',\n",
       " 'roasted',\n",
       " 'rock',\n",
       " 'roll',\n",
       " 'rolls',\n",
       " 'room',\n",
       " 'rooms',\n",
       " 'rude',\n",
       " 'run',\n",
       " 'running',\n",
       " 'sad',\n",
       " 'said',\n",
       " 'salad',\n",
       " 'salads',\n",
       " 'salmon',\n",
       " 'salsa',\n",
       " 'salt',\n",
       " 'salty',\n",
       " 'sample',\n",
       " 'sandwich',\n",
       " 'sandwiches',\n",
       " 'sat',\n",
       " 'saturday',\n",
       " 'sauce',\n",
       " 'sauces',\n",
       " 'sausage',\n",
       " 'save',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'school',\n",
       " 'scottsdale',\n",
       " 'seafood',\n",
       " 'seasoned',\n",
       " 'seat',\n",
       " 'seated',\n",
       " 'seating',\n",
       " 'seats',\n",
       " 'second',\n",
       " 'section',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'selection',\n",
       " 'sell',\n",
       " 'sense',\n",
       " 'seriously',\n",
       " 'serve',\n",
       " 'served',\n",
       " 'server',\n",
       " 'servers',\n",
       " 'service',\n",
       " 'serving',\n",
       " 'set',\n",
       " 'several',\n",
       " 'share',\n",
       " 'shared',\n",
       " 'shop',\n",
       " 'shopping',\n",
       " 'short',\n",
       " 'shot',\n",
       " 'show',\n",
       " 'showed',\n",
       " 'shrimp',\n",
       " 'side',\n",
       " 'sides',\n",
       " 'sign',\n",
       " 'similar',\n",
       " 'simple',\n",
       " 'simply',\n",
       " 'since',\n",
       " 'single',\n",
       " 'sit',\n",
       " 'sitting',\n",
       " 'size',\n",
       " 'slice',\n",
       " 'slices',\n",
       " 'slightly',\n",
       " 'slow',\n",
       " 'small',\n",
       " 'smaller',\n",
       " 'smell',\n",
       " 'soda',\n",
       " 'soft',\n",
       " 'solid',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometimes',\n",
       " 'somewhat',\n",
       " 'somewhere',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'sort',\n",
       " 'sound',\n",
       " 'soup',\n",
       " 'sour',\n",
       " 'space',\n",
       " 'special',\n",
       " 'specials',\n",
       " 'spend',\n",
       " 'spent',\n",
       " 'spice',\n",
       " 'spicy',\n",
       " 'spinach',\n",
       " 'split',\n",
       " 'sports',\n",
       " 'spot',\n",
       " 'spring',\n",
       " 'st',\n",
       " 'staff',\n",
       " 'stand',\n",
       " 'standard',\n",
       " 'star',\n",
       " 'stars',\n",
       " 'start',\n",
       " 'started',\n",
       " 'stay',\n",
       " 'stayed',\n",
       " 'steak',\n",
       " 'stick',\n",
       " 'still',\n",
       " 'stop',\n",
       " 'stopped',\n",
       " 'store',\n",
       " 'stores',\n",
       " 'street',\n",
       " 'strip',\n",
       " 'strong',\n",
       " 'stuff',\n",
       " 'stuffed',\n",
       " 'style',\n",
       " 'sugar',\n",
       " 'suggest',\n",
       " 'summer',\n",
       " 'sunday',\n",
       " 'super',\n",
       " 'supposed',\n",
       " 'sure',\n",
       " 'surprise',\n",
       " 'surprised',\n",
       " 'sushi',\n",
       " 'sweet',\n",
       " 'table',\n",
       " 'tables',\n",
       " 'taco',\n",
       " 'tacos',\n",
       " 'take',\n",
       " 'taken',\n",
       " 'takes',\n",
       " 'taking',\n",
       " 'talk',\n",
       " 'talking',\n",
       " 'tap',\n",
       " 'taste',\n",
       " 'tasted',\n",
       " 'tastes',\n",
       " 'tasting',\n",
       " 'tasty',\n",
       " 'tea',\n",
       " 'tell',\n",
       " 'tempe',\n",
       " 'tender',\n",
       " 'terrible',\n",
       " 'th',\n",
       " 'thai',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thats',\n",
       " 'theres',\n",
       " 'theyre',\n",
       " 'theyve',\n",
       " 'thick',\n",
       " 'thin',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thinking',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'three',\n",
       " 'throughout',\n",
       " 'time',\n",
       " 'times',\n",
       " 'tiny',\n",
       " 'tip',\n",
       " 'toast',\n",
       " 'today',\n",
       " 'together',\n",
       " 'told',\n",
       " 'tomato',\n",
       " 'tomatoes',\n",
       " 'ton',\n",
       " 'tons',\n",
       " 'took',\n",
       " 'top',\n",
       " 'toppings',\n",
       " 'tortilla',\n",
       " 'total',\n",
       " 'totally',\n",
       " 'touch',\n",
       " 'town',\n",
       " 'traditional',\n",
       " 'training',\n",
       " 'treat',\n",
       " 'tried',\n",
       " 'trip',\n",
       " 'true',\n",
       " 'truly',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'tuna',\n",
       " 'turkey',\n",
       " 'turned',\n",
       " 'tv',\n",
       " 'tvs',\n",
       " 'twice',\n",
       " 'two',\n",
       " 'type',\n",
       " 'typical',\n",
       " 'understand',\n",
       " 'unfortunately',\n",
       " 'unique',\n",
       " 'unless',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'using',\n",
       " 'usual',\n",
       " 'usually',\n",
       " 'valley',\n",
       " 'value',\n",
       " 'variety',\n",
       " 'vegetarian',\n",
       " 'veggie',\n",
       " 'veggies',\n",
       " 'vibe',\n",
       " 'view',\n",
       " 'visit',\n",
       " 'visited',\n",
       " 'visiting',\n",
       " 'w',\n",
       " 'wait',\n",
       " 'waited',\n",
       " 'waiter',\n",
       " 'waiting',\n",
       " 'waitress',\n",
       " 'walk',\n",
       " 'walked',\n",
       " 'walking',\n",
       " 'wall',\n",
       " 'walls',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'warm',\n",
       " 'wasnt',\n",
       " 'watch',\n",
       " 'watching',\n",
       " 'water',\n",
       " 'way',\n",
       " 'website',\n",
       " 'week',\n",
       " 'weekend',\n",
       " 'weekends',\n",
       " 'weeks',\n",
       " 'weird',\n",
       " 'well',\n",
       " 'went',\n",
       " 'werent',\n",
       " 'west',\n",
       " 'weve',\n",
       " 'whatever',\n",
       " 'whats',\n",
       " 'white',\n",
       " ...]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names() #give the list of features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm1=pd.DataFrame(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm1.columns=count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>across</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>added</th>\n",
       "      <th>addition</th>\n",
       "      <th>admit</th>\n",
       "      <th>afternoon</th>\n",
       "      <th>ago</th>\n",
       "      <th>...</th>\n",
       "      <th>yelp</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>youd</th>\n",
       "      <th>youll</th>\n",
       "      <th>young</th>\n",
       "      <th>youre</th>\n",
       "      <th>youve</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1036 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  absolutely  across  actually  add  added  addition  admit  afternoon  \\\n",
       "0     0           0       0         0    0      0         0      0          0   \n",
       "1     0           0       0         0    0      0         0      0          0   \n",
       "2     0           2       0         1    0      0         0      0          0   \n",
       "3     0           1       0         0    0      0         0      0          0   \n",
       "4     0           0       0         0    1      0         0      0          0   \n",
       "\n",
       "   ago  ...  yelp  yes  yet  youd  youll  young  youre  youve  yum  yummy  \n",
       "0    0  ...     0    0    0     0      0      0      0      0    0      0  \n",
       "1    0  ...     0    0    0     0      0      0      0      0    0      0  \n",
       "2    0  ...     0    0    0     0      0      0      0      0    0      0  \n",
       "3    0  ...     0    1    0     0      0      0      0      0    0      0  \n",
       "4    0  ...     0    0    1     0      0      0      0      0    0      0  \n",
       "\n",
       "[5 rows x 1036 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization (count, tfidf) for both train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "count_vect = CountVectorizer(analyzer='word', \n",
    "                             token_pattern=r'\\w{1,}', \n",
    "                             ngram_range=(1, 1 ), \n",
    "                             max_df = 0.9,\n",
    "                             min_df=0.01, \n",
    "                             encoding='latin-1')  #max_features=800\n",
    "\n",
    "xtrain_count = count_vect.fit_transform(X_train)\n",
    "\n",
    "\n",
    "#?TfidfVectorizer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(xtrain_count)\n",
    "\n",
    "#Test\n",
    "#count_vect = CountVectorizer()\n",
    "xtest_count = count_vect.transform(X_test)\n",
    "\n",
    "#tfidf_transformer = TfidfTransformer()\n",
    "X_test_tfidf = tfidf_transformer.transform(xtest_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm2=pd.DataFrame(X_train_tfidf.toarray(), columns=count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>across</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>added</th>\n",
       "      <th>addition</th>\n",
       "      <th>admit</th>\n",
       "      <th>afternoon</th>\n",
       "      <th>ago</th>\n",
       "      <th>...</th>\n",
       "      <th>yelp</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>youd</th>\n",
       "      <th>youll</th>\n",
       "      <th>young</th>\n",
       "      <th>youre</th>\n",
       "      <th>youve</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.235567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097861</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.279355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.254748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123697</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090637</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.424834</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.183000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1036 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  absolutely  across  actually       add  added  addition  admit  \\\n",
       "0   0.0    0.000000     0.0  0.000000  0.000000    0.0       0.0    0.0   \n",
       "1   0.0    0.000000     0.0  0.000000  0.000000    0.0       0.0    0.0   \n",
       "2   0.0    0.235567     0.0  0.097861  0.000000    0.0       0.0    0.0   \n",
       "3   0.0    0.279355     0.0  0.000000  0.000000    0.0       0.0    0.0   \n",
       "4   0.0    0.000000     0.0  0.000000  0.123697    0.0       0.0    0.0   \n",
       "5   0.0    0.000000     0.0  0.066656  0.000000    0.0       0.0    0.0   \n",
       "6   0.0    0.000000     0.0  0.000000  0.000000    0.0       0.0    0.0   \n",
       "7   0.0    0.000000     0.0  0.000000  0.000000    0.0       0.0    0.0   \n",
       "8   0.0    0.000000     0.0  0.000000  0.000000    0.0       0.0    0.0   \n",
       "9   0.0    0.000000     0.0  0.183000  0.000000    0.0       0.0    0.0   \n",
       "\n",
       "   afternoon  ago  ...  yelp       yes       yet  youd  youll  young  youre  \\\n",
       "0   0.000000  0.0  ...   0.0  0.000000  0.000000   0.0    0.0    0.0    0.0   \n",
       "1   0.000000  0.0  ...   0.0  0.000000  0.000000   0.0    0.0    0.0    0.0   \n",
       "2   0.000000  0.0  ...   0.0  0.000000  0.000000   0.0    0.0    0.0    0.0   \n",
       "3   0.000000  0.0  ...   0.0  0.254748  0.000000   0.0    0.0    0.0    0.0   \n",
       "4   0.000000  0.0  ...   0.0  0.000000  0.112291   0.0    0.0    0.0    0.0   \n",
       "5   0.090637  0.0  ...   0.0  0.000000  0.000000   0.0    0.0    0.0    0.0   \n",
       "6   0.000000  0.0  ...   0.0  0.000000  0.000000   0.0    0.0    0.0    0.0   \n",
       "7   0.000000  0.0  ...   0.0  0.000000  0.000000   0.0    0.0    0.0    0.0   \n",
       "8   0.000000  0.0  ...   0.0  0.000000  0.000000   0.0    0.0    0.0    0.0   \n",
       "9   0.000000  0.0  ...   0.0  0.000000  0.000000   0.0    0.0    0.0    0.0   \n",
       "\n",
       "   youve       yum  yummy  \n",
       "0    0.0  0.000000    0.0  \n",
       "1    0.0  0.000000    0.0  \n",
       "2    0.0  0.000000    0.0  \n",
       "3    0.0  0.000000    0.0  \n",
       "4    0.0  0.000000    0.0  \n",
       "5    0.0  0.000000    0.0  \n",
       "6    0.0  0.000000    0.0  \n",
       "7    0.0  0.424834    0.0  \n",
       "8    0.0  0.000000    0.0  \n",
       "9    0.0  0.000000    0.0  \n",
       "\n",
       "[10 rows x 1036 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm2.head(10) #inculing tdf-idm aswell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern='\\w{1,}', ngram_range=(1, 2), max_df = 0.9, min_df=0.01)\n",
    "tfidf_vect_ngram.fit(df['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "xtest_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create user defined function for train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier is type of modle,feature_vector_train is X_train, label is y_train, feature_vector_valid is X_test, valid_y is y_test\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid,  valid_y):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    return metrics.accuracy_score(classifier.predict(feature_vector_train), label), metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building different models with different vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of unique values of the said array:\n",
      "[[   1    2    3    4    5]\n",
      " [ 564  693 1096 2642 2505]]\n"
     ]
    }
   ],
   "source": [
    "# CONCEPT:- we have to build a multi-class modle---> convert to binary class becoz accurancy increased and imbalanced data\n",
    "\n",
    "unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of unique values of the said array:\n",
      "[[   1    2    3    4    5]\n",
      " [2642 2642 2642 2642 2642]]\n"
     ]
    }
   ],
   "source": [
    "#!pip install imblearn\n",
    "ros = RandomOverSampler(random_state=123)\n",
    "\n",
    "X_train_tfidf_os, y_train_tfidf_os = ros.fit_sample(X_train_tfidf, y_train)\n",
    "\n",
    "X_train_cnt_os, y_train_cnt_os = ros.fit_sample(xtrain_count, y_train)\n",
    "\n",
    "X_train_tfidf_ngram_os, y_train_tfidf_ngram_os = ros.fit_sample(xtrain_tfidf_ngram, y_train)\n",
    "\n",
    "unique_elements, counts_elements = np.unique(y_train_tfidf_os, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB  for L1, Count Vectors:  (0.6441332323996972, 0.4724)\n",
      "NB  for L1, WordLevel TF-IDF:  (0.6151400454201362, 0.488)\n",
      "NB  for L1, N-Gram Vectors:  (0.6430734292202877, 0.4724)\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "# Naive Bayes on TF-IDF\n",
    "accuracy_L1 = train_model(naive_bayes.MultinomialNB(), X_train_tfidf_os, y_train_tfidf_os, X_test_tfidf, y_test)\n",
    "print(\"NB  for L1, Count Vectors: \", accuracy_L1)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy_L1 = train_model(naive_bayes.MultinomialNB(), X_train_cnt_os, y_train_cnt_os, xtest_count, y_test)\n",
    "print(\"NB  for L1, WordLevel TF-IDF: \", accuracy_L1)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy_L1 = train_model(naive_bayes.MultinomialNB(), X_train_tfidf_ngram_os, y_train_tfidf_ngram_os, xtest_tfidf_ngram, y_test)\n",
    "print(\"NB  for L1, N-Gram Vectors: \", accuracy_L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR  for L1, Count Vectors:  (0.6820590461771385, 0.4672)\n",
      "LR  for L1, WordLevel TF-IDF:  (0.7205904617713853, 0.4556)\n",
      "LR  for L1, N-Gram Vectors:  (0.6024224072672218, 0.448)\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "# Logistic Regression on Count Vectors and TF-IDF\n",
    "accuracy_L1 = train_model(LogisticRegression(), X_train_tfidf_os, y_train_tfidf_os, X_test_tfidf, y_test)\n",
    "print(\"LR  for L1, Count Vectors: \", accuracy_L1)\n",
    "\n",
    "\n",
    "\n",
    "# Logistic Regression on Word Level TF IDF Vectors\n",
    "accuracy_L1 = train_model(LogisticRegression(), X_train_cnt_os, y_train_cnt_os, xtest_count, y_test)\n",
    "print(\"LR  for L1, WordLevel TF-IDF: \", accuracy_L1)\n",
    "\n",
    "\n",
    "\n",
    "# Logistic Regression on Ngram Level TF IDF Vectors\n",
    "accuracy_L1 = train_model(LogisticRegression(), X_train_tfidf_ngram_os, y_train_tfidf_ngram_os, xtest_tfidf_ngram, y_test)\n",
    "print(\"LR  for L1, N-Gram Vectors: \", accuracy_L1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC  for L1, Count Vectors:  (0.5281604844814535, 0.3552)\n",
      "SVC  for L1, WordLevel TF-IDF:  (0.6112036336109008, 0.4536)\n",
      "SVC  for L1, N-Gram Vectors:  (0.5299015897047691, 0.3672)\n"
     ]
    }
   ],
   "source": [
    "#Linear SVC\n",
    "# Linear SVC on Count Vectors and TF-IDF\n",
    "accuracy_L1 = train_model(SVC(), X_train_tfidf_os, y_train_tfidf_os, X_test_tfidf, y_test)\n",
    "print(\"SVC  for L1, Count Vectors: \", accuracy_L1)\n",
    "\n",
    "\n",
    "\n",
    "# Linear SVC on Word Level TF IDF Vectors\n",
    "accuracy_L1 = train_model(SVC(), X_train_cnt_os, y_train_cnt_os, xtest_count, y_test)\n",
    "print(\"SVC  for L1, WordLevel TF-IDF: \", accuracy_L1)\n",
    "\n",
    "\n",
    "\n",
    "# Linear SVC on Ngram Level TF IDF Vectors\n",
    "accuracy_L1 = train_model(SVC(), X_train_tfidf_ngram_os, y_train_tfidf_ngram_os, xtest_tfidf_ngram, y_test)\n",
    "print(\"SVC  for L1, N-Gram Vectors: \", accuracy_L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are not getting good accurancy by using Naive Bayes, SVM, Logistic. we are not able to fit mutliclass, \n",
    "           # so we have to convert to binary class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Features to a Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame that only contains the 5-star and 1-star reviews. converting the prob into binary class.\n",
    "\n",
    "yelp['stars'] = np.where(yelp['stars']>3, 5, np.where(yelp['stars']<3,1,3))\n",
    "\n",
    "yelp = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "\n",
    "# define X and y\n",
    "feature_cols = ['text', 'sentiment', 'cool', 'useful', 'funny']\n",
    "X = yelp[feature_cols]\n",
    "y = yelp.stars\n",
    "\n",
    "# split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6404, 986)\n",
      "(2135, 986)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6404, 4)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use CountVectorizer with text column only\n",
    "vect = TfidfVectorizer(lowercase=True, stop_words='english', max_df=0.9, min_df=0.01, ngram_range=(1, 2))\n",
    "X_train_dtm = vect.fit_transform(X_train.text)\n",
    "X_test_dtm = vect.transform(X_test.text)\n",
    "print(X_train_dtm.shape)\n",
    "print(X_test_dtm.shape)\n",
    "\n",
    "# shape of other four feature columns\n",
    "X_train.drop('text', axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6404, 24003)\n",
      "(2135, 24003)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6404, 4)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use CountVectorizer with text column only\n",
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train.text)\n",
    "X_test_dtm = vect.transform(X_test.text)\n",
    "print(X_train_dtm.shape)\n",
    "print(X_test_dtm.shape)\n",
    "\n",
    "# shape of other four feature columns\n",
    "X_train.drop('text', axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2135, 24007)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast other feature columns to float and convert to a sparse matrix\n",
    "extra = sparse.csr_matrix(X_train.drop('text', axis=1).astype(float))\n",
    "extra.shape\n",
    "\n",
    "# combine sparse matrices\n",
    "X_train_dtm_extra = sparse.hstack((X_train_dtm, extra))\n",
    "X_train_dtm_extra.shape\n",
    "\n",
    "# repeat for testing set\n",
    "extra = sparse.csr_matrix(X_test.drop('text', axis=1).astype(float))\n",
    "X_test_dtm_extra = sparse.hstack((X_test_dtm, extra))\n",
    "X_test_dtm_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression? #Bydefault L2 reg is working. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8880562060889929\n",
      "0.8233479105928085\n"
     ]
    }
   ],
   "source": [
    "# use logistic regression with text column only\n",
    "logreg = LogisticRegression(C=1e9)  # C is cost function, when C is 1 mean small cost function\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "print(metrics.roc_auc_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8946135831381733\n",
      "0.836418853255588\n"
     ]
    }
   ],
   "source": [
    "# use logistic regression with all features\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm_extra, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm_extra)\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "print(metrics.roc_auc_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to additional TextBlob features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob: \"Simplified Text Processing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have no idea why some people give bad reviews about this place. It goes to show you, you can please everyone. They are probably griping about something that their own fault...there are many people like that.\n",
      "\n",
      "In any case, my friend and I arrived at about 5:50 PM this past Sunday. It was pretty crowded, more than I thought for a Sunday evening and thought we would have to wait forever to get a seat but they said we'll be seated when the girl comes back from seating someone else. We were seated at 5:52 and the waiter came and got our drink orders. Everyone was very pleasant from the host that seated us to the waiter to the server. The prices were very good as well. We placed our orders once we decided what we wanted at 6:02. We shared the baked spaghetti calzone and the small \"Here's The Beef\" pizza so we can both try them. The calzone was huge and we got the smallest one (personal) and got the small 11\" pizza. Both were awesome! My friend liked the pizza better and I liked the calzone better. The calzone does have a sweetish sauce but that's how I like my sauce!\n",
      "\n",
      "We had to box part of the pizza to take it home and we were out the door by 6:42. So, everything was great and not like these bad reviewers. That goes to show you that  you have to try these things yourself because all these bad reviewers have some serious issues.\n"
     ]
    }
   ],
   "source": [
    "print(yelp.text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n",
      "\n",
      "Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n",
      "\n",
      "While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n",
      "\n",
      "Anyway, I can't wait to go back!\n"
     ]
    }
   ],
   "source": [
    "# print the first review\n",
    "print(yelp.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it as a TextBlob object\n",
    "review = TextBlob(yelp.text[0])\n",
    "#review1 = TextBlob('Good food, like it, excellent and recommended')\n",
    "#review1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40246913580246907"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_cmpkey', '_compare', '_create_sentence_objects', '_strkey', 'analyzer', 'classifier', 'classify', 'correct', 'detect_language', 'ends_with', 'endswith', 'find', 'format', 'index', 'join', 'json', 'lower', 'ngrams', 'noun_phrases', 'np_counts', 'np_extractor', 'parse', 'parser', 'polarity', 'pos_tagger', 'pos_tags', 'raw', 'raw_sentences', 'replace', 'rfind', 'rindex', 'sentences', 'sentiment', 'sentiment_assessments', 'serialized', 'split', 'starts_with', 'startswith', 'string', 'strip', 'stripped', 'subjectivity', 'tags', 'title', 'to_json', 'tokenize', 'tokenizer', 'tokens', 'translate', 'translator', 'upper', 'word_counts', 'words']\n"
     ]
    }
   ],
   "source": [
    "print(dir(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WordList(['My']), WordList(['wife']), WordList(['took']), WordList(['me']), WordList(['here']), WordList(['on']), WordList(['my']), WordList(['birthday']), WordList(['for']), WordList(['breakfast']), WordList(['and']), WordList(['it']), WordList(['was']), WordList(['excellent']), WordList(['The']), WordList(['weather']), WordList(['was']), WordList(['perfect']), WordList(['which']), WordList(['made']), WordList(['sitting']), WordList(['outside']), WordList(['overlooking']), WordList(['their']), WordList(['grounds']), WordList(['an']), WordList(['absolute']), WordList(['pleasure']), WordList(['Our']), WordList(['waitress']), WordList(['was']), WordList(['excellent']), WordList(['and']), WordList(['our']), WordList(['food']), WordList(['arrived']), WordList(['quickly']), WordList(['on']), WordList(['the']), WordList(['semi-busy']), WordList(['Saturday']), WordList(['morning']), WordList(['It']), WordList(['looked']), WordList(['like']), WordList(['the']), WordList(['place']), WordList(['fills']), WordList(['up']), WordList(['pretty']), WordList(['quickly']), WordList(['so']), WordList(['the']), WordList(['earlier']), WordList(['you']), WordList(['get']), WordList(['here']), WordList(['the']), WordList(['better']), WordList(['Do']), WordList(['yourself']), WordList(['a']), WordList(['favor']), WordList(['and']), WordList(['get']), WordList(['their']), WordList(['Bloody']), WordList(['Mary']), WordList(['It']), WordList(['was']), WordList(['phenomenal']), WordList(['and']), WordList(['simply']), WordList(['the']), WordList(['best']), WordList(['I']), WordList([\"'ve\"]), WordList(['ever']), WordList(['had']), WordList(['I']), WordList([\"'m\"]), WordList(['pretty']), WordList(['sure']), WordList(['they']), WordList(['only']), WordList(['use']), WordList(['ingredients']), WordList(['from']), WordList(['their']), WordList(['garden']), WordList(['and']), WordList(['blend']), WordList(['them']), WordList(['fresh']), WordList(['when']), WordList(['you']), WordList(['order']), WordList(['it']), WordList(['It']), WordList(['was']), WordList(['amazing']), WordList(['While']), WordList(['EVERYTHING']), WordList(['on']), WordList(['the']), WordList(['menu']), WordList(['looks']), WordList(['excellent']), WordList(['I']), WordList(['had']), WordList(['the']), WordList(['white']), WordList(['truffle']), WordList(['scrambled']), WordList(['eggs']), WordList(['vegetable']), WordList(['skillet']), WordList(['and']), WordList(['it']), WordList(['was']), WordList(['tasty']), WordList(['and']), WordList(['delicious']), WordList(['It']), WordList(['came']), WordList(['with']), WordList(['2']), WordList(['pieces']), WordList(['of']), WordList(['their']), WordList(['griddled']), WordList(['bread']), WordList(['with']), WordList(['was']), WordList(['amazing']), WordList(['and']), WordList(['it']), WordList(['absolutely']), WordList(['made']), WordList(['the']), WordList(['meal']), WordList(['complete']), WordList(['It']), WordList(['was']), WordList(['the']), WordList(['best']), WordList(['toast']), WordList(['I']), WordList([\"'ve\"]), WordList(['ever']), WordList(['had']), WordList(['Anyway']), WordList(['I']), WordList(['ca']), WordList([\"n't\"]), WordList(['wait']), WordList(['to']), WordList(['go']), WordList(['back'])]\n"
     ]
    }
   ],
   "source": [
    "print(review.ngrams(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40246913580246907"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excellent', 'The', 'weather', 'was', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'grounds', 'an', 'absolute', 'pleasure', 'Our', 'waitress', 'was', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'looked', 'like', 'the', 'place', 'fills', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', 'was', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'had', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', 'was', 'amazing', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'looks', 'excellent', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'and', 'it', 'was', 'tasty', 'and', 'delicious', 'It', 'came', 'with', '2', 'pieces', 'of', 'their', 'griddled', 'bread', 'with', 'was', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'It', 'was', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'had', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the words\n",
    "review.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"My wife took me here on my birthday for breakfast and it was excellent.\"),\n",
       " Sentence(\"The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.\"),\n",
       " Sentence(\"Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.\"),\n",
       " Sentence(\"It looked like the place fills up pretty quickly so the earlier you get here the better.\"),\n",
       " Sentence(\"Do yourself a favor and get their Bloody Mary.\"),\n",
       " Sentence(\"It was phenomenal and simply the best I've ever had.\"),\n",
       " Sentence(\"I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.\"),\n",
       " Sentence(\"It was amazing.\"),\n",
       " Sentence(\"While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.\"),\n",
       " Sentence(\"It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.\"),\n",
       " Sentence(\"It was the best \"toast\" I've ever had.\"),\n",
       " Sentence(\"Anyway, I can't wait to go back!\")]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the sentences\n",
    "review.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"my wife took me here on my birthday for breakfast and it was excellent.  the weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  our waitress was excellent and our food arrived quickly on the semi-busy saturday morning.  it looked like the place fills up pretty quickly so the earlier you get here the better.\n",
       "\n",
       "do yourself a favor and get their bloody mary.  it was phenomenal and simply the best i've ever had.  i'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  it was amazing.\n",
       "\n",
       "while everything on the menu looks excellent, i had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  it came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  it was the best \"toast\" i've ever had.\n",
       "\n",
       "anyway, i can't wait to go back!\")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some string methods are available\n",
    "review.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['I', 'have']),\n",
       " WordList(['have', 'no']),\n",
       " WordList(['no', 'idea']),\n",
       " WordList(['idea', 'why']),\n",
       " WordList(['why', 'some']),\n",
       " WordList(['some', 'people']),\n",
       " WordList(['people', 'give']),\n",
       " WordList(['give', 'bad']),\n",
       " WordList(['bad', 'reviews']),\n",
       " WordList(['reviews', 'about']),\n",
       " WordList(['about', 'this']),\n",
       " WordList(['this', 'place']),\n",
       " WordList(['place', 'It']),\n",
       " WordList(['It', 'goes']),\n",
       " WordList(['goes', 'to']),\n",
       " WordList(['to', 'show']),\n",
       " WordList(['show', 'you']),\n",
       " WordList(['you', 'you']),\n",
       " WordList(['you', 'can']),\n",
       " WordList(['can', 'please']),\n",
       " WordList(['please', 'everyone']),\n",
       " WordList(['everyone', 'They']),\n",
       " WordList(['They', 'are']),\n",
       " WordList(['are', 'probably']),\n",
       " WordList(['probably', 'griping']),\n",
       " WordList(['griping', 'about']),\n",
       " WordList(['about', 'something']),\n",
       " WordList(['something', 'that']),\n",
       " WordList(['that', 'their']),\n",
       " WordList(['their', 'own']),\n",
       " WordList(['own', 'fault']),\n",
       " WordList(['fault', 'there']),\n",
       " WordList(['there', 'are']),\n",
       " WordList(['are', 'many']),\n",
       " WordList(['many', 'people']),\n",
       " WordList(['people', 'like']),\n",
       " WordList(['like', 'that']),\n",
       " WordList(['that', 'In']),\n",
       " WordList(['In', 'any']),\n",
       " WordList(['any', 'case']),\n",
       " WordList(['case', 'my']),\n",
       " WordList(['my', 'friend']),\n",
       " WordList(['friend', 'and']),\n",
       " WordList(['and', 'I']),\n",
       " WordList(['I', 'arrived']),\n",
       " WordList(['arrived', 'at']),\n",
       " WordList(['at', 'about']),\n",
       " WordList(['about', '5:50']),\n",
       " WordList(['5:50', 'PM']),\n",
       " WordList(['PM', 'this']),\n",
       " WordList(['this', 'past']),\n",
       " WordList(['past', 'Sunday']),\n",
       " WordList(['Sunday', 'It']),\n",
       " WordList(['It', 'was']),\n",
       " WordList(['was', 'pretty']),\n",
       " WordList(['pretty', 'crowded']),\n",
       " WordList(['crowded', 'more']),\n",
       " WordList(['more', 'than']),\n",
       " WordList(['than', 'I']),\n",
       " WordList(['I', 'thought']),\n",
       " WordList(['thought', 'for']),\n",
       " WordList(['for', 'a']),\n",
       " WordList(['a', 'Sunday']),\n",
       " WordList(['Sunday', 'evening']),\n",
       " WordList(['evening', 'and']),\n",
       " WordList(['and', 'thought']),\n",
       " WordList(['thought', 'we']),\n",
       " WordList(['we', 'would']),\n",
       " WordList(['would', 'have']),\n",
       " WordList(['have', 'to']),\n",
       " WordList(['to', 'wait']),\n",
       " WordList(['wait', 'forever']),\n",
       " WordList(['forever', 'to']),\n",
       " WordList(['to', 'get']),\n",
       " WordList(['get', 'a']),\n",
       " WordList(['a', 'seat']),\n",
       " WordList(['seat', 'but']),\n",
       " WordList(['but', 'they']),\n",
       " WordList(['they', 'said']),\n",
       " WordList(['said', 'we']),\n",
       " WordList(['we', \"'ll\"]),\n",
       " WordList([\"'ll\", 'be']),\n",
       " WordList(['be', 'seated']),\n",
       " WordList(['seated', 'when']),\n",
       " WordList(['when', 'the']),\n",
       " WordList(['the', 'girl']),\n",
       " WordList(['girl', 'comes']),\n",
       " WordList(['comes', 'back']),\n",
       " WordList(['back', 'from']),\n",
       " WordList(['from', 'seating']),\n",
       " WordList(['seating', 'someone']),\n",
       " WordList(['someone', 'else']),\n",
       " WordList(['else', 'We']),\n",
       " WordList(['We', 'were']),\n",
       " WordList(['were', 'seated']),\n",
       " WordList(['seated', 'at']),\n",
       " WordList(['at', '5:52']),\n",
       " WordList(['5:52', 'and']),\n",
       " WordList(['and', 'the']),\n",
       " WordList(['the', 'waiter']),\n",
       " WordList(['waiter', 'came']),\n",
       " WordList(['came', 'and']),\n",
       " WordList(['and', 'got']),\n",
       " WordList(['got', 'our']),\n",
       " WordList(['our', 'drink']),\n",
       " WordList(['drink', 'orders']),\n",
       " WordList(['orders', 'Everyone']),\n",
       " WordList(['Everyone', 'was']),\n",
       " WordList(['was', 'very']),\n",
       " WordList(['very', 'pleasant']),\n",
       " WordList(['pleasant', 'from']),\n",
       " WordList(['from', 'the']),\n",
       " WordList(['the', 'host']),\n",
       " WordList(['host', 'that']),\n",
       " WordList(['that', 'seated']),\n",
       " WordList(['seated', 'us']),\n",
       " WordList(['us', 'to']),\n",
       " WordList(['to', 'the']),\n",
       " WordList(['the', 'waiter']),\n",
       " WordList(['waiter', 'to']),\n",
       " WordList(['to', 'the']),\n",
       " WordList(['the', 'server']),\n",
       " WordList(['server', 'The']),\n",
       " WordList(['The', 'prices']),\n",
       " WordList(['prices', 'were']),\n",
       " WordList(['were', 'very']),\n",
       " WordList(['very', 'good']),\n",
       " WordList(['good', 'as']),\n",
       " WordList(['as', 'well']),\n",
       " WordList(['well', 'We']),\n",
       " WordList(['We', 'placed']),\n",
       " WordList(['placed', 'our']),\n",
       " WordList(['our', 'orders']),\n",
       " WordList(['orders', 'once']),\n",
       " WordList(['once', 'we']),\n",
       " WordList(['we', 'decided']),\n",
       " WordList(['decided', 'what']),\n",
       " WordList(['what', 'we']),\n",
       " WordList(['we', 'wanted']),\n",
       " WordList(['wanted', 'at']),\n",
       " WordList(['at', '6:02']),\n",
       " WordList(['6:02', 'We']),\n",
       " WordList(['We', 'shared']),\n",
       " WordList(['shared', 'the']),\n",
       " WordList(['the', 'baked']),\n",
       " WordList(['baked', 'spaghetti']),\n",
       " WordList(['spaghetti', 'calzone']),\n",
       " WordList(['calzone', 'and']),\n",
       " WordList(['and', 'the']),\n",
       " WordList(['the', 'small']),\n",
       " WordList(['small', 'Here']),\n",
       " WordList(['Here', \"'s\"]),\n",
       " WordList([\"'s\", 'The']),\n",
       " WordList(['The', 'Beef']),\n",
       " WordList(['Beef', 'pizza']),\n",
       " WordList(['pizza', 'so']),\n",
       " WordList(['so', 'we']),\n",
       " WordList(['we', 'can']),\n",
       " WordList(['can', 'both']),\n",
       " WordList(['both', 'try']),\n",
       " WordList(['try', 'them']),\n",
       " WordList(['them', 'The']),\n",
       " WordList(['The', 'calzone']),\n",
       " WordList(['calzone', 'was']),\n",
       " WordList(['was', 'huge']),\n",
       " WordList(['huge', 'and']),\n",
       " WordList(['and', 'we']),\n",
       " WordList(['we', 'got']),\n",
       " WordList(['got', 'the']),\n",
       " WordList(['the', 'smallest']),\n",
       " WordList(['smallest', 'one']),\n",
       " WordList(['one', 'personal']),\n",
       " WordList(['personal', 'and']),\n",
       " WordList(['and', 'got']),\n",
       " WordList(['got', 'the']),\n",
       " WordList(['the', 'small']),\n",
       " WordList(['small', '11']),\n",
       " WordList(['11', 'pizza']),\n",
       " WordList(['pizza', 'Both']),\n",
       " WordList(['Both', 'were']),\n",
       " WordList(['were', 'awesome']),\n",
       " WordList(['awesome', 'My']),\n",
       " WordList(['My', 'friend']),\n",
       " WordList(['friend', 'liked']),\n",
       " WordList(['liked', 'the']),\n",
       " WordList(['the', 'pizza']),\n",
       " WordList(['pizza', 'better']),\n",
       " WordList(['better', 'and']),\n",
       " WordList(['and', 'I']),\n",
       " WordList(['I', 'liked']),\n",
       " WordList(['liked', 'the']),\n",
       " WordList(['the', 'calzone']),\n",
       " WordList(['calzone', 'better']),\n",
       " WordList(['better', 'The']),\n",
       " WordList(['The', 'calzone']),\n",
       " WordList(['calzone', 'does']),\n",
       " WordList(['does', 'have']),\n",
       " WordList(['have', 'a']),\n",
       " WordList(['a', 'sweetish']),\n",
       " WordList(['sweetish', 'sauce']),\n",
       " WordList(['sauce', 'but']),\n",
       " WordList(['but', 'that']),\n",
       " WordList(['that', \"'s\"]),\n",
       " WordList([\"'s\", 'how']),\n",
       " WordList(['how', 'I']),\n",
       " WordList(['I', 'like']),\n",
       " WordList(['like', 'my']),\n",
       " WordList(['my', 'sauce']),\n",
       " WordList(['sauce', 'We']),\n",
       " WordList(['We', 'had']),\n",
       " WordList(['had', 'to']),\n",
       " WordList(['to', 'box']),\n",
       " WordList(['box', 'part']),\n",
       " WordList(['part', 'of']),\n",
       " WordList(['of', 'the']),\n",
       " WordList(['the', 'pizza']),\n",
       " WordList(['pizza', 'to']),\n",
       " WordList(['to', 'take']),\n",
       " WordList(['take', 'it']),\n",
       " WordList(['it', 'home']),\n",
       " WordList(['home', 'and']),\n",
       " WordList(['and', 'we']),\n",
       " WordList(['we', 'were']),\n",
       " WordList(['were', 'out']),\n",
       " WordList(['out', 'the']),\n",
       " WordList(['the', 'door']),\n",
       " WordList(['door', 'by']),\n",
       " WordList(['by', '6:42']),\n",
       " WordList(['6:42', 'So']),\n",
       " WordList(['So', 'everything']),\n",
       " WordList(['everything', 'was']),\n",
       " WordList(['was', 'great']),\n",
       " WordList(['great', 'and']),\n",
       " WordList(['and', 'not']),\n",
       " WordList(['not', 'like']),\n",
       " WordList(['like', 'these']),\n",
       " WordList(['these', 'bad']),\n",
       " WordList(['bad', 'reviewers']),\n",
       " WordList(['reviewers', 'That']),\n",
       " WordList(['That', 'goes']),\n",
       " WordList(['goes', 'to']),\n",
       " WordList(['to', 'show']),\n",
       " WordList(['show', 'you']),\n",
       " WordList(['you', 'that']),\n",
       " WordList(['that', 'you']),\n",
       " WordList(['you', 'have']),\n",
       " WordList(['have', 'to']),\n",
       " WordList(['to', 'try']),\n",
       " WordList(['try', 'these']),\n",
       " WordList(['these', 'things']),\n",
       " WordList(['things', 'yourself']),\n",
       " WordList(['yourself', 'because']),\n",
       " WordList(['because', 'all']),\n",
       " WordList(['all', 'these']),\n",
       " WordList(['these', 'bad']),\n",
       " WordList(['bad', 'reviewers']),\n",
       " WordList(['reviewers', 'have']),\n",
       " WordList(['have', 'some']),\n",
       " WordList(['some', 'serious']),\n",
       " WordList(['serious', 'issues'])]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.ngrams(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language correction, detection, translation etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"15 minutes late\")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spelling correction\n",
    "TextBlob('15 minuets late').correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"this is my email adcresc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"this is my email address\")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(s).correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('part', 0.9929478138222849), ('parrot', 0.007052186177715092)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spellcheck\n",
    "Word('parot').spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tip laterally',\n",
       " 'enclose with a bank',\n",
       " 'do business with a bank or keep an account at a bank',\n",
       " 'act as the banker in a game or in gambling',\n",
       " 'be in the banking business',\n",
       " 'put into a bank account',\n",
       " 'cover with ashes so to control the rate of burning',\n",
       " 'have confidence or faith in']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# definitions\n",
    "Word('bank').define('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'te'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# language detection\n",
    "TextBlob('టీమిండియా మాజీ ఓపెనర్‌ గౌతమ్ గంభీర్ మంగళవారం క్రికెట్‌కు రిటైర్‌మెంట్ ప్రకటించారు. దానిపై స్పందించిన బాలీవుడ్ స్టార్‌ షారుక్‌ ఖాన్‌ ఓ ట్వీట్ చేశారు').detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El ex abridor de India Gautam Gambhir anunció su retiro del cricket el martes En respuesta la estrella de Bollywood Shah Rukh Khan tuiteó\n"
     ]
    }
   ],
   "source": [
    "# Language Translation\n",
    "a=' '.join(TextBlob('టీమిండియా మాజీ ఓపెనర్‌ గౌతమ్ గంభీర్ మంగళవారం క్రికెట్‌కు రిటైర్‌మెంట్ ప్రకటించారు. దానిపై స్పందించిన బాలీవుడ్ స్టార్‌ షారుక్‌ ఖాన్‌ ఓ ట్వీట్ చేశారు').translate(to='es').words)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming:**\n",
    "\n",
    "- **What:** Reduce a word to its base/stem/root form\n",
    "- **Why:** Often makes sense to treat related words the same way\n",
    "- **Notes:**\n",
    "    - Uses a \"simple\" and fast rule-based approach\n",
    "    - Stemmed words are usually not shown to users (used for analysis/indexing)\n",
    "    - Some search engines treat words with the same stem as synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.stem.snowball.SnowballStemmer at 0x1a338eacf8>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize stemmer\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excellent', 'The', 'weather', 'was', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'grounds', 'an', 'absolute', 'pleasure', 'Our', 'waitress', 'was', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'looked', 'like', 'the', 'place', 'fills', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', 'was', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'had', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', 'was', 'amazing', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'looks', 'excellent', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'and', 'it', 'was', 'tasty', 'and', 'delicious', 'It', 'came', 'with', '2', 'pieces', 'of', 'their', 'griddled', 'bread', 'with', 'was', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'It', 'was', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'had', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back'])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excel', 'the', 'weather', 'was', 'perfect', 'which', 'made', 'sit', 'outsid', 'overlook', 'their', 'ground', 'an', 'absolut', 'pleasur', 'our', 'waitress', 'was', 'excel', 'and', 'our', 'food', 'arriv', 'quick', 'on', 'the', 'semi-busi', 'saturday', 'morn', 'it', 'look', 'like', 'the', 'place', 'fill', 'up', 'pretti', 'quick', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'bloodi', 'mari', 'it', 'was', 'phenomen', 'and', 'simpli', 'the', 'best', 'i', 've', 'ever', 'had', 'i', \"'m\", 'pretti', 'sure', 'they', 'onli', 'use', 'ingredi', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'it', 'was', 'amaz', 'while', 'everyth', 'on', 'the', 'menu', 'look', 'excel', 'i', 'had', 'the', 'white', 'truffl', 'scrambl', 'egg', 'veget', 'skillet', 'and', 'it', 'was', 'tasti', 'and', 'delici', 'it', 'came', 'with', '2', 'piec', 'of', 'their', 'griddl', 'bread', 'with', 'was', 'amaz', 'and', 'it', 'absolut', 'made', 'the', 'meal', 'complet', 'it', 'was', 'the', 'best', 'toast', 'i', 've', 'ever', 'had', 'anyway', 'i', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "# stem each word\n",
    "print([stemmer.stem(word) for word in review.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**\n",
    "\n",
    "- **What:** Derive the canonical form ('lemma') of a word\n",
    "- **Why:** Can be better than stemming\n",
    "- **Notes:** Uses a dictionary-based approach (slower than stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excellent', 'The', 'weather', 'was', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'grounds', 'an', 'absolute', 'pleasure', 'Our', 'waitress', 'was', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'looked', 'like', 'the', 'place', 'fills', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', 'was', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'had', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', 'was', 'amazing', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'looks', 'excellent', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'and', 'it', 'was', 'tasty', 'and', 'delicious', 'It', 'came', 'with', '2', 'pieces', 'of', 'their', 'griddled', 'bread', 'with', 'was', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'It', 'was', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'had', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back'])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'wa', 'excellent', 'The', 'weather', 'wa', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'ground', 'an', 'absolute', 'pleasure', 'Our', 'waitress', 'wa', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'looked', 'like', 'the', 'place', 'fill', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', 'wa', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'had', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredient', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', 'wa', 'amazing', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'look', 'excellent', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', 'egg', 'vegetable', 'skillet', 'and', 'it', 'wa', 'tasty', 'and', 'delicious', 'It', 'came', 'with', '2', 'piece', 'of', 'their', 'griddled', 'bread', 'with', 'wa', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'It', 'wa', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'had', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "# assume every word is a noun\n",
    "print([word.lemmatize() for word in review.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'wife', 'take', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'be', 'excellent', 'The', 'weather', 'be', 'perfect', 'which', 'make', 'sit', 'outside', 'overlook', 'their', 'ground', 'an', 'absolute', 'pleasure', 'Our', 'waitress', 'be', 'excellent', 'and', 'our', 'food', 'arrive', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'look', 'like', 'the', 'place', 'fill', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', 'be', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'have', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', 'be', 'amaze', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'look', 'excellent', 'I', 'have', 'the', 'white', 'truffle', 'scramble', 'egg', 'vegetable', 'skillet', 'and', 'it', 'be', 'tasty', 'and', 'delicious', 'It', 'come', 'with', '2', 'piece', 'of', 'their', 'griddle', 'bread', 'with', 'be', 'amaze', 'and', 'it', 'absolutely', 'make', 'the', 'meal', 'complete', 'It', 'be', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'have', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "# assume every word is a verb\n",
    "print([word.lemmatize(pos='v') for word in review.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns a list of lemmas\n",
    "def split_into_lemmas(text):\n",
    "    #text = unicode(text, 'utf-8').lower()\n",
    "    text = text.lower()\n",
    "    words = TextBlob(text).words\n",
    "    return [word.lemmatize() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  27212\n",
      "Accuracy:  0.4624\n"
     ]
    }
   ],
   "source": [
    "# use split_into_lemmas as the feature extraction function (WARNING: SLOW!)\n",
    "vect = CountVectorizer(analyzer=split_into_lemmas)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zinc', 'zinfandel', 'zing', 'zip', 'zipcar', 'ziploc', 'zipper', 'zipps', 'zippy', 'ziti', 'zoes', 'zoey', 'zoeys', 'zoftik', 'zola', 'zombie', 'zombielike', 'zone', 'zoned', 'zoners', 'zoning', 'zoo', 'zoom', 'zoomed', 'zoyo', 'zpizza', 'ztejas', 'ztrip', 'zucca', 'zucchini', 'zucchinimushroom', 'zuccini', 'zuch', 'zuchinni', 'zuma', 'zumaroka', 'zumba', 'zupa', 'zur', 'zuzu', 'zuzus', 'zweigel', 'zwiebelkräuter', 'zzzzzzzzzzzzzzzzz', '¡cash', '¡excellent', '¡muy', '¢', '°', 'école']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "print(vect.get_feature_names()[-50:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
