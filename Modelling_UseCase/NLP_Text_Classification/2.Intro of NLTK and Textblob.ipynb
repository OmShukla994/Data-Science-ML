{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nltk book is here: http://www.nltk.org/book/\n",
    "\n",
    "NLP applications: http://blog.mashape.com/list-of-25-natural-language-processing-apis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have a sentence, and we want to extract all the words from it.\n",
    "sentence = \"Camera quality is not upto the mark I visited one plus store and the store representative checked my phone and compared the camera quality with his demo phone He noticed a significant difference between the two I request Amazon to replace my phone with a new one as it is evident that this phone has some problems\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Camera',\n",
       " 'quality',\n",
       " 'is',\n",
       " 'not',\n",
       " 'upto',\n",
       " 'the',\n",
       " 'mark',\n",
       " 'I',\n",
       " 'visited',\n",
       " 'one',\n",
       " 'plus',\n",
       " 'store',\n",
       " 'and',\n",
       " 'the',\n",
       " 'store',\n",
       " 'representative',\n",
       " 'checked',\n",
       " 'my',\n",
       " 'phone',\n",
       " 'and',\n",
       " 'compared',\n",
       " 'the',\n",
       " 'camera',\n",
       " 'quality',\n",
       " 'with',\n",
       " 'his',\n",
       " 'demo',\n",
       " 'phone',\n",
       " 'He',\n",
       " 'noticed',\n",
       " 'a',\n",
       " 'significant',\n",
       " 'difference',\n",
       " 'between',\n",
       " 'the',\n",
       " 'two',\n",
       " 'I',\n",
       " 'request',\n",
       " 'Amazon',\n",
       " 'to',\n",
       " 'replace',\n",
       " 'my',\n",
       " 'phone',\n",
       " 'with',\n",
       " 'a',\n",
       " 'new',\n",
       " 'one',\n",
       " 'as',\n",
       " 'it',\n",
       " 'is',\n",
       " 'evident',\n",
       " 'that',\n",
       " 'this',\n",
       " 'phone',\n",
       " 'has',\n",
       " 'some',\n",
       " 'problems']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['camera',\n",
       " 'quality',\n",
       " 'is',\n",
       " 'not',\n",
       " 'upto',\n",
       " 'the',\n",
       " 'mark',\n",
       " 'i',\n",
       " 'visited',\n",
       " 'one',\n",
       " 'plus',\n",
       " 'store',\n",
       " 'and',\n",
       " 'the',\n",
       " 'store',\n",
       " 'representative',\n",
       " 'checked',\n",
       " 'my',\n",
       " 'phone',\n",
       " 'and',\n",
       " 'compared',\n",
       " 'the',\n",
       " 'camera',\n",
       " 'quality',\n",
       " 'with',\n",
       " 'his',\n",
       " 'demo',\n",
       " 'phone',\n",
       " 'he',\n",
       " 'noticed',\n",
       " 'a',\n",
       " 'significant',\n",
       " 'difference',\n",
       " 'between',\n",
       " 'the',\n",
       " 'two',\n",
       " 'i',\n",
       " 'request',\n",
       " 'amazon',\n",
       " 'to',\n",
       " 'replace',\n",
       " 'my',\n",
       " 'phone',\n",
       " 'with',\n",
       " 'a',\n",
       " 'new',\n",
       " 'one',\n",
       " 'as',\n",
       " 'it',\n",
       " 'is',\n",
       " 'evident',\n",
       " 'that',\n",
       " 'this',\n",
       " 'phone',\n",
       " 'has',\n",
       " 'some',\n",
       " 'problems']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can split the function on a space (” “) to get all the words. \n",
    "# However, The problem with this is, we cannot extract punctuation marks like full stops, \n",
    "# and this simple parser will not be able to handle every single type of sentence.\n",
    "\n",
    "# Which is why we should use the word tokenizer provided by the NLTK library. This correctly identifies punctuation marks:\n",
    "from nltk.tokenize import word_tokenize\n",
    "sentence = sentence.lower()\n",
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aber',\n",
       " 'alle',\n",
       " 'allem',\n",
       " 'allen',\n",
       " 'aller',\n",
       " 'alles',\n",
       " 'als',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ander',\n",
       " 'andere',\n",
       " 'anderem',\n",
       " 'anderen',\n",
       " 'anderer',\n",
       " 'anderes',\n",
       " 'anderm',\n",
       " 'andern',\n",
       " 'anderr',\n",
       " 'anders',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'aus',\n",
       " 'bei',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bist',\n",
       " 'da',\n",
       " 'damit',\n",
       " 'dann',\n",
       " 'der',\n",
       " 'den',\n",
       " 'des',\n",
       " 'dem',\n",
       " 'die',\n",
       " 'das',\n",
       " 'dass',\n",
       " 'daß',\n",
       " 'derselbe',\n",
       " 'derselben',\n",
       " 'denselben',\n",
       " 'desselben',\n",
       " 'demselben',\n",
       " 'dieselbe',\n",
       " 'dieselben',\n",
       " 'dasselbe',\n",
       " 'dazu',\n",
       " 'dein',\n",
       " 'deine',\n",
       " 'deinem',\n",
       " 'deinen',\n",
       " 'deiner',\n",
       " 'deines',\n",
       " 'denn',\n",
       " 'derer',\n",
       " 'dessen',\n",
       " 'dich',\n",
       " 'dir',\n",
       " 'du',\n",
       " 'dies',\n",
       " 'diese',\n",
       " 'diesem',\n",
       " 'diesen',\n",
       " 'dieser',\n",
       " 'dieses',\n",
       " 'doch',\n",
       " 'dort',\n",
       " 'durch',\n",
       " 'ein',\n",
       " 'eine',\n",
       " 'einem',\n",
       " 'einen',\n",
       " 'einer',\n",
       " 'eines',\n",
       " 'einig',\n",
       " 'einige',\n",
       " 'einigem',\n",
       " 'einigen',\n",
       " 'einiger',\n",
       " 'einiges',\n",
       " 'einmal',\n",
       " 'er',\n",
       " 'ihn',\n",
       " 'ihm',\n",
       " 'es',\n",
       " 'etwas',\n",
       " 'euer',\n",
       " 'eure',\n",
       " 'eurem',\n",
       " 'euren',\n",
       " 'eurer',\n",
       " 'eures',\n",
       " 'für',\n",
       " 'gegen',\n",
       " 'gewesen',\n",
       " 'hab',\n",
       " 'habe',\n",
       " 'haben',\n",
       " 'hat',\n",
       " 'hatte',\n",
       " 'hatten',\n",
       " 'hier',\n",
       " 'hin',\n",
       " 'hinter',\n",
       " 'ich',\n",
       " 'mich',\n",
       " 'mir',\n",
       " 'ihr',\n",
       " 'ihre',\n",
       " 'ihrem',\n",
       " 'ihren',\n",
       " 'ihrer',\n",
       " 'ihres',\n",
       " 'euch',\n",
       " 'im',\n",
       " 'in',\n",
       " 'indem',\n",
       " 'ins',\n",
       " 'ist',\n",
       " 'jede',\n",
       " 'jedem',\n",
       " 'jeden',\n",
       " 'jeder',\n",
       " 'jedes',\n",
       " 'jene',\n",
       " 'jenem',\n",
       " 'jenen',\n",
       " 'jener',\n",
       " 'jenes',\n",
       " 'jetzt',\n",
       " 'kann',\n",
       " 'kein',\n",
       " 'keine',\n",
       " 'keinem',\n",
       " 'keinen',\n",
       " 'keiner',\n",
       " 'keines',\n",
       " 'können',\n",
       " 'könnte',\n",
       " 'machen',\n",
       " 'man',\n",
       " 'manche',\n",
       " 'manchem',\n",
       " 'manchen',\n",
       " 'mancher',\n",
       " 'manches',\n",
       " 'mein',\n",
       " 'meine',\n",
       " 'meinem',\n",
       " 'meinen',\n",
       " 'meiner',\n",
       " 'meines',\n",
       " 'mit',\n",
       " 'muss',\n",
       " 'musste',\n",
       " 'nach',\n",
       " 'nicht',\n",
       " 'nichts',\n",
       " 'noch',\n",
       " 'nun',\n",
       " 'nur',\n",
       " 'ob',\n",
       " 'oder',\n",
       " 'ohne',\n",
       " 'sehr',\n",
       " 'sein',\n",
       " 'seine',\n",
       " 'seinem',\n",
       " 'seinen',\n",
       " 'seiner',\n",
       " 'seines',\n",
       " 'selbst',\n",
       " 'sich',\n",
       " 'sie',\n",
       " 'ihnen',\n",
       " 'sind',\n",
       " 'so',\n",
       " 'solche',\n",
       " 'solchem',\n",
       " 'solchen',\n",
       " 'solcher',\n",
       " 'solches',\n",
       " 'soll',\n",
       " 'sollte',\n",
       " 'sondern',\n",
       " 'sonst',\n",
       " 'über',\n",
       " 'um',\n",
       " 'und',\n",
       " 'uns',\n",
       " 'unsere',\n",
       " 'unserem',\n",
       " 'unseren',\n",
       " 'unser',\n",
       " 'unseres',\n",
       " 'unter',\n",
       " 'viel',\n",
       " 'vom',\n",
       " 'von',\n",
       " 'vor',\n",
       " 'während',\n",
       " 'war',\n",
       " 'waren',\n",
       " 'warst',\n",
       " 'was',\n",
       " 'weg',\n",
       " 'weil',\n",
       " 'weiter',\n",
       " 'welche',\n",
       " 'welchem',\n",
       " 'welchen',\n",
       " 'welcher',\n",
       " 'welches',\n",
       " 'wenn',\n",
       " 'werde',\n",
       " 'werden',\n",
       " 'wie',\n",
       " 'wieder',\n",
       " 'will',\n",
       " 'wir',\n",
       " 'wird',\n",
       " 'wirst',\n",
       " 'wo',\n",
       " 'wollen',\n",
       " 'wollte',\n",
       " 'würde',\n",
       " 'würden',\n",
       " 'zu',\n",
       " 'zum',\n",
       " 'zur',\n",
       " 'zwar',\n",
       " 'zwischen']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nltk comes inbuilt with a list of stop words for all main languages. \n",
    "# To see the stop words for English:\n",
    "\n",
    "\n",
    "stopwords.words('german')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "\n",
    "Remove all punct\n",
    "Identify the parts of speech "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('camera', 'NN'),\n",
       " ('quality', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('not', 'RB'),\n",
       " ('upto', 'JJ'),\n",
       " ('the', 'DT'),\n",
       " ('mark', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('visited', 'VBD'),\n",
       " ('one', 'CD'),\n",
       " ('plus', 'CC'),\n",
       " ('store', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('store', 'NN'),\n",
       " ('representative', 'NN'),\n",
       " ('checked', 'VBD'),\n",
       " ('my', 'PRP$'),\n",
       " ('phone', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('compared', 'VBN'),\n",
       " ('the', 'DT'),\n",
       " ('camera', 'NN'),\n",
       " ('quality', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('his', 'PRP$'),\n",
       " ('demo', 'NN'),\n",
       " ('phone', 'NN'),\n",
       " ('he', 'PRP'),\n",
       " ('noticed', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('significant', 'JJ'),\n",
       " ('difference', 'NN'),\n",
       " ('between', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('two', 'CD'),\n",
       " ('i', 'NN'),\n",
       " ('request', 'VBP'),\n",
       " ('amazon', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('replace', 'VB'),\n",
       " ('my', 'PRP$'),\n",
       " ('phone', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('new', 'JJ'),\n",
       " ('one', 'CD'),\n",
       " ('as', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('evident', 'JJ'),\n",
       " ('that', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('phone', 'NN'),\n",
       " ('has', 'VBZ'),\n",
       " ('some', 'DT'),\n",
       " ('problems', 'NNS')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "# Now, let's get a tag associated with each and every token and see what part of speech these are.\n",
    "# Whether they're noun, pronoun, adverb, adjective etc.\n",
    "\n",
    "# By doing so, we can learn more about the constituents of a statement/tweet and see what kind of worlds are \n",
    "# present in it.\n",
    "w = word_tokenize(sentence)\n",
    "tokensLC = list()\n",
    "for words in w:\n",
    "    tokensLC.append(words.lower())\n",
    "\n",
    "nltk.pos_tag(tokensLC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of tags: http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('guerrilla.n.01'), Synset('irregular.n.02'), Synset('irregular.a.01'), Synset('irregular.s.02'), Synset('irregular.a.03'), Synset('irregular.a.04'), Synset('irregular.s.05'), Synset('atypical.s.02'), Synset('irregular.s.07'), Synset('irregular.s.08'), Synset('irregular.s.09')]\n",
      "guerrilla.n.01\n",
      "a member of an irregular armed force that fights a stronger force by sabotage and harassment\n",
      "irregular.n.02\n",
      "merchandise that has imperfections; usually sold at a reduced price without the brand name\n"
     ]
    }
   ],
   "source": [
    "# The Nltk has many great features, like finding the meaning of words, finding examples of words, \n",
    "# finding similar and opposite words etc. \n",
    "\n",
    "# You can see how useful these features would be if you were building like a search engine, or a text parser.\n",
    "\n",
    "# Let’s look at a few of these features.\n",
    "\n",
    "# The first thing you can do it, find the definition of any word.\n",
    "syn = wordnet.synsets(\"irregular\")\n",
    "print(syn)\n",
    "print(syn[0].name())\n",
    "print(syn[0].definition())\n",
    "\n",
    "print(syn[1].name())\n",
    "print(syn[1].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the map showed roads and other features',\n",
       " 'generosity is one of his best characteristics']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn = wordnet.synsets(\"feature\")\n",
    "syn[0].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he did four sets of the incline bench press']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn = wordnet.synsets(\"set\")\n",
    "syn[1].examples()\n",
    "syn[2].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('communicate.v.02')]\n",
      "**********************************************\n",
      "[Synset('babble.v.01'), Synset('bark.v.01'), Synset('bay.v.01'), Synset('begin.v.04'), Synset('blubber.v.02'), Synset('blurt_out.v.01'), Synset('bumble.v.03'), Synset('cackle.v.01'), Synset('chatter.v.04'), Synset('chatter.v.05'), Synset('deliver.v.01'), Synset('drone.v.02'), Synset('enthuse.v.02'), Synset('generalize.v.02'), Synset('gulp.v.02'), Synset('hiss.v.03'), Synset('lip_off.v.01'), Synset('mumble.v.01'), Synset('murmur.v.01'), Synset('open_up.v.07'), Synset('peep.v.04'), Synset('rant.v.01'), Synset('rasp.v.02'), Synset('read.v.03'), Synset('shout.v.01'), Synset('sing.v.02'), Synset('slur.v.03'), Synset('snap.v.01'), Synset('snivel.v.01'), Synset('speak_in_tongues.v.01'), Synset('speak_up.v.02'), Synset('swallow.v.04'), Synset('talk_of.v.01'), Synset('tone.v.01'), Synset('tone.v.02'), Synset('troll.v.07'), Synset('verbalize.v.01'), Synset('vocalize.v.05'), Synset('whiff.v.05'), Synset('whisper.v.01'), Synset('yack.v.01')]\n"
     ]
    }
   ],
   "source": [
    "# We can get words closer to a certain word using synsets, hypernyms and hyponymns.\n",
    "# Eg., we use the word \"Speak\" here. and what we get is the synonymns of speak.\n",
    "\n",
    "# Hymernym : a word with a broad meaning constituting a category into which words with more \n",
    "#            specific meanings fall\n",
    "\n",
    "# Hyponyms : each of two or more words having the same spelling or pronunciation but \n",
    "#            different meanings and origins \n",
    "\n",
    "syn = wordnet.synsets(\"speak\")[0]\n",
    "print(syn.hypernyms())\n",
    "print(\"**********************************************\")\n",
    "print(syn.hyponyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma('talk.v.02.talk')\n",
      "Lemma('talk.v.02.speak')\n",
      "Lemma('talk.v.02.utter')\n",
      "Lemma('talk.v.02.mouth')\n",
      "Lemma('talk.v.02.verbalize')\n",
      "Lemma('talk.v.02.verbalise')\n",
      "Lemma('talk.v.01.talk')\n",
      "Lemma('talk.v.01.speak')\n",
      "Lemma('speak.v.03.speak')\n",
      "Lemma('speak.v.03.talk')\n",
      "Lemma('address.v.02.address')\n",
      "Lemma('address.v.02.speak')\n",
      "Lemma('speak.v.05.speak')\n"
     ]
    }
   ],
   "source": [
    "# Here in this example, we try to find the opposite words of \"good\"\n",
    "\n",
    "# lemma in NLTK is a canonical form of a word.\n",
    "syn = wordnet.synsets(\"speak\")\n",
    "\n",
    "for s in syn:\n",
    "    for l in s.lemmas():\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************************************************\n",
      "Synonyms of book\n",
      "-------------------------------------------------------------------------------------\n",
      "[Synset('book.n.01'), Synset('book.n.02'), Synset('record.n.05'), Synset('script.n.01'), Synset('ledger.n.01'), Synset('book.n.06'), Synset('book.n.07'), Synset('koran.n.01'), Synset('bible.n.01'), Synset('book.n.10'), Synset('book.n.11'), Synset('book.v.01'), Synset('reserve.v.04'), Synset('book.v.03'), Synset('book.v.04')]\n",
      "-------------------------------------------------------------------------------------\n",
      "*************************************************************************************\n",
      "Lemmas of book - Words that are similar to the word book and unique words in NLTK\n",
      "-------------------------------------------------------------------------------------\n",
      "[Lemma('book.n.01.book')]\n",
      "[Lemma('book.n.02.book'), Lemma('book.n.02.volume')]\n",
      "[Lemma('record.n.05.record'), Lemma('record.n.05.record_book'), Lemma('record.n.05.book')]\n",
      "[Lemma('script.n.01.script'), Lemma('script.n.01.book'), Lemma('script.n.01.playscript')]\n",
      "[Lemma('ledger.n.01.ledger'), Lemma('ledger.n.01.leger'), Lemma('ledger.n.01.account_book'), Lemma('ledger.n.01.book_of_account'), Lemma('ledger.n.01.book')]\n",
      "[Lemma('book.n.06.book')]\n",
      "[Lemma('book.n.07.book'), Lemma('book.n.07.rule_book')]\n",
      "[Lemma('koran.n.01.Koran'), Lemma('koran.n.01.Quran'), Lemma('koran.n.01.al-Qur'an'), Lemma('koran.n.01.Book')]\n",
      "[Lemma('bible.n.01.Bible'), Lemma('bible.n.01.Christian_Bible'), Lemma('bible.n.01.Book'), Lemma('bible.n.01.Good_Book'), Lemma('bible.n.01.Holy_Scripture'), Lemma('bible.n.01.Holy_Writ'), Lemma('bible.n.01.Scripture'), Lemma('bible.n.01.Word_of_God'), Lemma('bible.n.01.Word')]\n",
      "[Lemma('book.n.10.book')]\n",
      "[Lemma('book.n.11.book')]\n",
      "[Lemma('book.v.01.book')]\n",
      "[Lemma('reserve.v.04.reserve'), Lemma('reserve.v.04.hold'), Lemma('reserve.v.04.book')]\n",
      "[Lemma('book.v.03.book')]\n",
      "[Lemma('book.v.04.book')]\n",
      "-------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Lemmas can be used to find all similar words:\n",
    "\n",
    "# And it heps us to reduce/substitute a set of words to one single word\n",
    "\n",
    "\n",
    "syn = wordnet.synsets(\"book\")\n",
    "print(\"*************************************************************************************\")\n",
    "print(\"Synonyms of book\")\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "print(syn)\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"*************************************************************************************\")\n",
    "print(\"Lemmas of book - Words that are similar to the word book and unique words in NLTK\")\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "for s in syn:\n",
    "    print(s.lemmas())\n",
    "print(\"-------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob import Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"15 minutes late\")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spelling correction\n",
    "TextBlob('15 minuten late').correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"this is bc\")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=\"this is bcz\"\n",
    "TextBlob(s).correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('study', 0.9664429530201343),\n",
       " ('stud', 0.020134228187919462),\n",
       " ('studio', 0.013422818791946308)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word('studi').spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TextBlob' object has no attribute 'detect_language'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m TextBlob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis is a bright day\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdetect_language()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TextBlob' object has no attribute 'detect_language'"
     ]
    }
   ],
   "source": [
    "TextBlob('This is a bright day').detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob('Aaj bahut acha din hai #AchcheDin').detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'te'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob('Chala manchi roju idi').detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm looking forward to this\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob.translator.translate('Aaj bahut shubh din hai',to_lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************************************************\n",
      "Words with all stopwords\n",
      "-------------------------------------------------------------------------------------\n",
      "['AnalytixLabs', '-', 'leading', 'Capability', 'Building', 'and', 'Training', 'Solutions', 'Provider', '.', 'Our', 'courses', 'are', 'crafted', 'by', 'experts', 'to', 'keep', 'you', 'ahead', 'of', 'the', 'curve', 'in', 'industry', 'best', 'practices', '.', 'Case', 'study', 'based', 'modules', 'ensure', 'that', 'participants', 'learn', 'practical', 'applications', 'along', 'with', 'the', 'theoretical', 'concepts', '.', 'Further', 'to', 'this', ',', 'new', 'courses', 'are', 'continuously', 'launched', 'and', 'old', 'ones', 'keep', 'evolving', 'as', 'per', 'the', 'latest', 'and', 'upcoming', 'industry', 'trends', '.', 'High', 'degree', 'of', 'commitment', '&', 'personal', 'attention', 'is', 'given', 'through', 'small', 'batch', 'size', 'and', 'individual', 'counselling', '.', 'Hands-on', 'sessions', 'and', 'practice', 'assignments', 'on', 'real', 'life', 'business', 'datasets', 'are', 'included', 'to', 'ensure', 'assimilated', 'learning', '.']\n",
      "-------------------------------------------------------------------------------------\n",
      "*************************************************************************************\n",
      "Sentence is clean now - no stop words included\n",
      "-------------------------------------------------------------------------------------\n",
      "['analytixlabs', '-', 'leading', 'capability', 'building', 'training', 'solutions', 'provider', '.', 'our', 'courses', 'crafted', 'experts', 'keep', 'ahead', 'curve', 'industry', 'best', 'practices', '.', 'case', 'study', 'based', 'modules', 'ensure', 'participants', 'learn', 'practical', 'applications', 'along', 'theoretical', 'concepts', '.', 'further', ',', 'new', 'courses', 'continuously', 'launched', 'old', 'ones', 'keep', 'evolving', 'per', 'latest', 'upcoming', 'industry', 'trends', '.', 'high', 'degree', 'commitment', '&', 'personal', 'attention', 'given', 'small', 'batch', 'size', 'individual', 'counselling', '.', 'hands-on', 'sessions', 'practice', 'assignments', 'real', 'life', 'business', 'datasets', 'included', 'ensure', 'assimilated', 'learning', '.']\n",
      "-------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# https://en.wikipedia.org/wiki/Cadet_Nurse_Corps\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "para = \"\"\"\n",
    "AnalytixLabs - leading Capability Building and Training Solutions Provider.\n",
    "\n",
    "Our courses are crafted by experts to keep you ahead of the curve in industry best practices. \n",
    "Case study based modules ensure that participants learn practical applications along with the theoretical concepts. \n",
    "\n",
    "Further to this, new courses are continuously launched and old ones keep evolving as per the latest and upcoming \n",
    "industry trends.\n",
    "\n",
    "High degree of commitment & personal attention is given through small batch size and individual counselling. \n",
    "Hands-on sessions and practice assignments on real life business datasets are included to ensure assimilated learning.\n",
    "\n",
    "\"\"\"\n",
    "words = word_tokenize(para)\n",
    "print(\"*************************************************************************************\")\n",
    "print(\"Words with all stopwords\")\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "print(words)\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "\n",
    "useful_words = [word.lower() for word in words if word not in stopwords.words('english')]\n",
    "print(\"*************************************************************************************\")\n",
    "print(\"Sentence is clean now - no stop words included\")\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "print(useful_words)\n",
    "print(\"-------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quick': True, 'brown': True, 'fox': True}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is how the Naive Bayes classifier expects the input\n",
    "\n",
    "def create_word_features(words):\n",
    "    useful_words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    my_dict = dict([(word, True) for word in useful_words])\n",
    "    return my_dict\n",
    "\n",
    "create_word_features([\"the\", \"quick\", \"brown\", \"quick\", \"a\", \"fox\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abbrevations and Words correction\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:{}`+=~|.!?,'0-9]\", \"\", text)\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedWords = []\n",
    "for words in useful_words:\n",
    "    \n",
    "    cleanedWords.append(clean_text(words))\n",
    "cleanedWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P\n",
      "U\n",
      "B\n",
      "G\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Player's Unknown Battle Grounds\"\n",
    "s = \"\"\n",
    "for i in sentence.split(\" \"):\n",
    "    print(i[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
